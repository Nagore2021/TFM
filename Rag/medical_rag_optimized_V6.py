"""
medical_rag_simple_clear.py - RAG M√©dico Simple y Claro

OBJETIVO: Sistema f√°cil de entender sin confusi√≥n
PROCESO: Pregunta ‚Üí Pipeline H√≠brido ‚Üí Mejor Chunk ‚Üí Respuesta M√©dica
"""

import os
import sys
import logging
from typing import Dict, Any, Optional
from dataclasses import dataclass
import time

# Imports b√°sicos
import torch
from transformers import pipeline

# Importar la clase existente
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)

from retrieval.bm25_model_chunk_bge import BM25DualChunkEvaluator
from embeddings.load_model import cargar_configuracion

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

@dataclass
class SimpleMedicalResponse:
    """Respuesta m√©dica simple"""
    question: str
    answer: str
    chunk_used: Dict[str, Any]
    processing_time: float
    success: bool

class SimpleMedicalRAG:
    """
    RAG M√©dico Simple y Claro con Pipeline H√≠brido
    

    1. Usuario hace pregunta m√©dica
    2. Pipeline H√≠brido busca el chunk m√°s relevante (BM25 + Bi-Encoder + Cross-Encoder)
    3. Se muestra qu√© chunk encontr√≥
    4. Se genera respuesta m√©dica usando ese chunk
    5. Se muestra la respuesta final
    """
    
    def __init__(self, config_path: str, mode: str = "embedding"):
        """Inicializa RAG m√©dico simple"""
        
        self.config_path = config_path
        self.mode = mode
        
        self.config = cargar_configuracion(config_path)
        
        # Componentes del sistema
        self.retrieval_system = None
        self.generation_pipeline = None
        self.is_initialized = False
    

    def initialize(self) -> bool:
        """Inicializa el sistema de forma simple"""
        try:
           
            
            # 1. Cargar sistema BM25
            print("Cargando base de conocimientos m√©dicos...")
            self.retrieval_system = BM25DualChunkEvaluator(self.config_path, self.mode)
            self.retrieval_system.load_collection()
            
            total_chunks = len(self.retrieval_system.chunk_ids)
           
            # 2. Cargar modelo de generaci√≥n
            print("Cargando modelo de respuestas...")
            self._load_generation_model()
            
            self.is_initialized = True
            # print("Sistema listo para consultas m√©dicas\n")
            
            return True
            
        except Exception as e:
            print(f"Error inicializando: {e}")
            return False

    def _load_generation_model(self):
        """Carga modelo de generaci√≥n optimizado para medicina"""
        try:
            device = 0 if torch.cuda.is_available() else -1
            
            
            model_candidates = [
                "microsoft/DialoGPT-large"
            ]
            
            for model_name in model_candidates:
                try:
                    print(f"Probando modelo: {model_name}")
                    
                    self.generation_pipeline = pipeline(
                        "text-generation",
                        model=model_name,
                        device=device,
                        max_length=1024,  # Contexto amplio para prompts m√©dicos
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        trust_remote_code=True
                    )
                    
                    if self.generation_pipeline.tokenizer.pad_token is None:
                        self.generation_pipeline.tokenizer.pad_token = self.generation_pipeline.tokenizer.eos_token

                   
                    break
                    
                except Exception as model_error:
                    print(f" Error con {model_name}: {model_error}")
                    continue
            
            if self.generation_pipeline is None:
                print(" No se pudo cargar ning√∫n modelo")
                
        except Exception as e:
            print(f"Error general cargando modelos: {e}")
            print("Usando modo respuestas estructuradas como m√©dico")
            self.generation_pipeline = None

    def ask_doctor(self, medical_question: str) -> SimpleMedicalResponse:
        """
        Pregunta al doctor 
        """
        start_time = time.time()
        
        if not self.is_initialized:
            return SimpleMedicalResponse(
                question=medical_question,
                answer="Sistema no inicializado",
                chunk_used={},
                processing_time=0.0,
                success=False
            )
        
        
        try:
            # PASO 1: Buscar informaci√≥n m√©dica relevante
            print(" PASO 1: Buscando informaci√≥n m√©dica relevante...")
            chunk_info = self._find_best_medical_chunk(medical_question)
            
            if not chunk_info:
                return SimpleMedicalResponse(
                    question=medical_question,
                    answer="No encontr√© informaci√≥n m√©dica relevante para su consulta. Le recomiendo consultar con su m√©dico.",
                    chunk_used={},
                    processing_time=time.time() - start_time,
                    success=False
                )
            
            # PASO 2: Mostrar qu√© informaci√≥n se encontr√≥
            self._show_chunk_info(chunk_info)
            
            # PASO 3: Generar respuesta m√©dica
            print("\PASO 3: Generando respuesta m√©dica...")
            medical_answer = self._generate_medical_answer(medical_question, chunk_info)
            
            processing_time = time.time() - start_time
            
            response = SimpleMedicalResponse(
                question=medical_question,
                answer=medical_answer,
                chunk_used=chunk_info,
                processing_time=processing_time,
                success=True
            )
            
            # PASO 4: Mostrar respuesta final
            self._show_final_response(response)
            
            return response
            
        except Exception as e:
            print(f"Error del sistema: {str(e)}")
            return SimpleMedicalResponse(
                question=medical_question,
                answer=f"Error del sistema: {str(e)}",
                chunk_used={},
                processing_time=time.time() - start_time,
                success=False
            )

    def _find_best_medical_chunk(self, question: str) -> Optional[Dict[str, Any]]:
        """Busca el mejor fragmento m√©dico usando Pipeline H√≠brido - Confianza total en el algoritmo"""
        try:
            print("Usando Pipeline H√≠brido: BM25 + Bi-Encoder + Cross-Encoder...")
            
            # Pipeline H√≠brido Completo - Sin filtros adicionales
            hybrid_results = self.retrieval_system.calculate_hybrid_pipeline(
                query=question, 
                pool_size=10,    # Pool balanceado de 10 chunks
                batch_size=8     # Procesamiento eficiente
            )
            
            if not hybrid_results:
                print("Pipeline h√≠brido vac√≠o, usando solo BM25...")
                # Fallback a solo BM25
                hybrid_results = self.retrieval_system.calculate_bm25_rankings(question)
            
            if not hybrid_results:
                return None
            
            # Confiar en el resultado del pipeline h√≠brido
            best_chunk = hybrid_results[0]
            print(f"Mejor chunk seg√∫n pipeline h√≠brido: {best_chunk}")
            
            chunk_text = self.retrieval_system.docs_raw.get(best_chunk, '')
            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk, {})
            
            return {
                "chunk_id": best_chunk,
                "text": chunk_text,
                "filename": chunk_metadata.get('filename', 'Gu√≠a m√©dica'),
                "document_id": chunk_metadata.get('document_id', ''),
                "chunk_position": chunk_metadata.get('chunk_position', ''),
                "categoria": chunk_metadata.get('categoria', 'medicina'),
                "strategy_used": "Pipeline H√≠brido (BM25+Bi-Encoder+Cross-Encoder)",
                "all_results": hybrid_results[:5]  # Top 5 para mostrar
            }
            
        except Exception as e:
            print(f"Error en b√∫squeda h√≠brida: {e}")
            return self._find_best_medical_chunk_bm25_only(question)

    def _find_best_medical_chunk_bm25_only(self, question: str) -> Optional[Dict[str, Any]]:
        """Fallback: Busca usando solo BM25 - Sin filtros, confianza en el algoritmo"""
        try:
            # Usar BM25 para encontrar candidatos
            bm25_results = self.retrieval_system.calculate_bm25_rankings(question)
            
            if not bm25_results:
                return None
            
            # Confiar en el primer resultado de BM25
            best_chunk = bm25_results[0]
            print(f"Mejor chunk seg√∫n BM25: {best_chunk}")
            
            chunk_text = self.retrieval_system.docs_raw.get(best_chunk, '')
            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk, {})
            
            return {
                "chunk_id": best_chunk,
                "text": chunk_text,
                "filename": chunk_metadata.get('filename', 'Gu√≠a m√©dica'),
                "document_id": chunk_metadata.get('document_id', ''),
                "chunk_position": chunk_metadata.get('chunk_position', ''),
                "categoria": chunk_metadata.get('categoria', 'medicina'),
                "strategy_used": "BM25 ranking directo",
                "all_results": bm25_results[:5]  # Top 5 para mostrar
            }
            
        except Exception as e:
            print(f"Error en b√∫squeda BM25: {e}")
            return None

    def _show_chunk_info(self, chunk_info: Dict[str, Any]):
        """Muestra informaci√≥n clara del chunk encontrado"""
        print(f" INFORMACI√ìN ENCONTRADA:")
        print(f" Estrategia: {chunk_info.get('strategy_used', 'No especificada')}")
        print(f" Documento: {chunk_info['document_id']}")
        print(f" Archivo: {chunk_info['filename']}")
        print(f" Posici√≥n: {chunk_info['chunk_position']}")
        print(f"  Categor√≠a: {chunk_info['categoria']}")
        print(f" Tama√±o: {len(chunk_info['text'])} caracteres")
        
        # Mostrar preview del contenido
        preview = chunk_info['text'][:300] + "..." if len(chunk_info['text']) > 300 else chunk_info['text']
        print(f"  Contenido: {preview}")
        
        # Mostrar alternativas consideradas
        print(f"  Top 3 resultados: {chunk_info['all_results'][:3]}")
        print(f"  Estrategia: {chunk_info.get('strategy_used', 'No especificada')}")

    def _generate_medical_answer(self, question: str, chunk_info: Dict[str, Any]) -> str:
        """Genera respuesta m√©dica usando el chunk encontrado - SOLO respuestas estructuradas"""
        
        chunk_text = chunk_info['text']
        source = chunk_info['filename']
        
        # DECISI√ìN: El modelo DialoGPT no funciona bien para medicina
        # Usar SOLO respuestas estructuradas que son m√°s confiables
        print("üí° Usando respuesta m√©dica estructurada (m√°s confiable que modelo)")
        return self._create_structured_answer(question, chunk_text, source)



    def _create_structured_answer(self, question: str, context: str, source: str) -> str:
        """Crea respuesta estructurada como m√©dico de atenci√≥n primaria cuando falla el modelo"""
        
        context_preview = context[:500] + "..." if len(context) > 500 else context
        question_lower = question.lower()
        
        # Respuesta como m√©dico de atenci√≥n primaria
        medical_intro = "Como m√©dico de atenci√≥n primaria, comprendo su preocupaci√≥n y quiero ayudarle."
        
        # Adaptar respuesta seg√∫n tipo de consulta
        if any(word in question_lower for word in ['dolor de cabeza', 'cefalea', 'migra√±a']):
            specific_advice = """
 EVALUACI√ìN INICIAL:
Los dolores de cabeza frecuentes requieren una evaluaci√≥n m√©dica adecuada para determinar su causa y el tratamiento m√°s apropiado.

 RECOMENDACIONES INMEDIATAS:
‚Ä¢ Mantenga un diario de cefaleas: anote cu√°ndo ocurren, intensidad (1-10), duraci√≥n y posibles desencadenantes
‚Ä¢ Aseg√∫rese de mantener una hidrataci√≥n adecuada y patrones de sue√±o regulares
‚Ä¢ Evite factores desencadenantes comunes como estr√©s, ayuno prolongado o ciertos alimentos

 SIGNOS DE ALARMA - BUSQUE ATENCI√ìN INMEDIATA SI PRESENTA:
‚Ä¢ Dolor de cabeza severo y s√∫bito ("el peor de su vida")
‚Ä¢ Cefalea acompa√±ada de fiebre, rigidez de cuello o alteraci√≥n de la conciencia
‚Ä¢ Cambios en la visi√≥n, debilidad o dificultad para hablar
‚Ä¢ Dolor de cabeza que empeora progresivamente"""

        elif any(word in question_lower for word in ['diabetes', 'az√∫car', 'sed', 'orinar']):
            specific_advice = """
 EVALUACI√ìN INICIAL:
Los s√≠ntomas que describe pueden sugerir alteraciones en los niveles de glucosa y requieren evaluaci√≥n m√©dica.

 RECOMENDACIONES INMEDIATAS:
‚Ä¢ Programe una cita para realizarse an√°lisis de glucosa en sangre en ayunas
‚Ä¢ Mantenga un registro de s√≠ntomas: sed, micci√≥n frecuente, cambios en el apetito
‚Ä¢ Contin√∫e con una dieta equilibrada y ejercicio moderado seg√∫n su capacidad

 SIGNOS DE ALARMA - BUSQUE ATENCI√ìN INMEDIATA SI PRESENTA:
‚Ä¢ N√°useas o v√≥mitos persistentes
‚Ä¢ Dificultad para respirar o dolor abdominal intenso
‚Ä¢ Confusi√≥n o alteraci√≥n del nivel de conciencia
‚Ä¢ Deshidrataci√≥n severa"""

        else:
            specific_advice = """
 EVALUACI√ìN INICIAL:
Bas√°ndome en su consulta y la informaci√≥n m√©dica disponible, le proporciono las siguientes recomendaciones.

 RECOMENDACIONES GENERALES:
‚Ä¢ Para una evaluaci√≥n personalizada, programe una cita en consulta
‚Ä¢ Mantenga un registro de sus s√≠ntomas para facilitar el diagn√≥stico
‚Ä¢ Siga las medidas generales de cuidado de la salud

 SIGNOS DE ALARMA - BUSQUE ATENCI√ìN INMEDIATA SI PRESENTA:
‚Ä¢ S√≠ntomas severos o que empeoran r√°pidamente
‚Ä¢ Dificultad respiratoria o dolor tor√°cico
‚Ä¢ Alteraci√≥n del nivel de conciencia o s√≠ntomas neurol√≥gicos"""

        return f"""{medical_intro}

 INFORMACI√ìN M√âDICA RELEVANTE:
{context_preview}

{specific_advice}

 PR√ìXIMOS PASOS:
‚Ä¢ Programe una cita en consulta para evaluaci√≥n presencial
‚Ä¢ Traiga consigo cualquier medicaci√≥n actual y resultados de estudios previos
‚Ä¢ No dude en contactar si presenta s√≠ntomas de alarma

 *Esta respuesta est√° basada en: {source}*

‚Ñπ Recuerde: Esta informaci√≥n es de car√°cter educativo y no reemplaza la consulta m√©dica presencial. Para un diagn√≥stico preciso y tratamiento personalizado, es fundamental la evaluaci√≥n cl√≠nica directa."""

    def _show_final_response(self, response: SimpleMedicalResponse):
        """Muestra la respuesta final de forma clara"""
        print(f"\n RESPUESTA M√âDICA:")
        print("="*60)
        print(response.answer)
        print("="*60)
        print(f" Tiempo de procesamiento: {response.processing_time:.2f} segundos")


# main.py - Punto de entrada para el sistema RAG m√©dico simple
def main():
  

    print("Estrategia: BM25 + Bi-Encoder + Cross-Encoder")
    
    print("="*60)
    
    # Inicializar sistema
    rag = SimpleMedicalRAG("../config.yaml", mode="finetuning")
    
    if not rag.initialize():
        print("Error en inicializaci√≥n")
        return
    
    # Consultas de prueba simples
    test_questions = [
        "¬øCu√°les son los s√≠ntomas de la diabetes?",
        "Doctor, tengo dolor de cabeza frecuente",
        "¬øQu√© puedo hacer para la presi√≥n alta?"
    ]
  
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n PRUEBA {i}/{len(test_questions)}")
        response = rag.ask_doctor(question)
        
        if not response.success:
            print(f"Error en consulta: {response.answer}")
        
        # Pausa entre consultas para claridad
        if i < len(test_questions):
            input("\n Presiona Enter para continuar con la siguiente consulta...")
    

if __name__ == "__main__":
    main()