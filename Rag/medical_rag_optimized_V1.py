"""
medical_rag_optimized_fast.py - RAG M√©dico Optimizado para VELOCIDAD

Optimizaciones principales:
1. Reduce pool_size de 50 a 10-15 chunks
2. Caching de embeddings frecuentes  
3. Early stopping en cross-encoder
4. Mistral con menos tokens
5. BM25 con menos candidatos
"""

import os
import sys
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
import pickle
import hashlib

# Imports para Mistral
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Importar la clase existente
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)

from retrieval.bm25_model_chunk_bge import BM25DualChunkEvaluator
from embeddings.load_model import cargar_configuracion

# Configurar logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

@dataclass
class MedicalConsultation:
    """Consulta m√©dica completa con respuesta"""
    question: str
    answer: str
    best_chunk: Dict[str, Any]
    pipeline_stats: Dict[str, int]
    success: bool
    processing_time: float = 0.0

class FastMedicalRAG:
    """
    RAG M√©dico Optimizado para VELOCIDAD
    
    Optimizaciones implementadas:
    - Pool reducido (10-15 chunks vs 50)
    - Cache de embeddings frecuentes
    - Early stopping en cross-encoder
    - Mistral con par√°metros m√°s r√°pidos
    - BM25 limitado a top 10
    """
    
    def __init__(self, config_path: str, mode: str = "embedding"):
        """Inicializa RAG m√©dico optimizado para velocidad"""
        
        self.config_path = config_path
        
        # ========= PAR√ÅMETROS ULTRA-OPTIMIZADOS PARA VELOCIDAD =========
        # OPCI√ìN 1: Hardware lento (respuesta en ~2-3min)
        if mode == "ultra_fast":
            self.pool_size = 5           # M√≠nimo viable
            self.bm25_top_k = 4          
            self.biencoder_top_k = 4      
            self.max_new_tokens = 150    # Respuestas muy cortas
            self.skip_cross_encoder = True  # Saltar cross-encoder
        else:
            # OPCI√ìN 2: Balance velocidad-calidad (respuesta en ~3-4min)
            self.pool_size = 10           # Reducido de 50 a 10
            self.bm25_top_k = 8           # Reduci
        
        # Cache para embeddings frecuentes
        self.query_cache = {}
        self.cache_file = "query_embeddings_cache.pkl"
        self._load_cache()
        
        try:
            self.config = cargar_configuracion(config_path)
        except Exception:
            logger.warning("‚ö†Ô∏è No se pudo cargar config.yaml, usando valores por defecto.")
            self.config = {}

        self.mode = mode
        
        # Configurar modelos con par√°metros r√°pidos
        default_models_dir = os.path.join(PROJECT_ROOT, 'models')
        base_path = self.config.get('model_path', default_models_dir)
        llm = self.config.get('models', {}).get('llm_model', None)
        
        if llm:
            self.mistral_model_name = os.path.join(base_path, llm)
        else:
            # Modelo m√°s peque√±o como fallback
            self.mistral_model_name = "microsoft/DialoGPT-medium"
        
        # Componentes
        self.retrieval_system = None
        self.mistral_pipeline = None
        self.is_initialized = False

        logger.info("‚ö° RAG M√©dico FAST - Optimizado para velocidad")
        logger.info(f"üéØ Pool size: {self.pool_size}, BM25 top-k: {self.bm25_top_k}")

    def _load_cache(self):
        """Carga cache de embeddings de queries frecuentes"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'rb') as f:
                    self.query_cache = pickle.load(f)
                logger.info(f"üì¶ Cache cargado: {len(self.query_cache)} queries")
            else:
                self.query_cache = {}
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error cargando cache: {e}")
            self.query_cache = {}

    def _save_cache(self):
        """Guarda cache de embeddings"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.query_cache, f)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error guardando cache: {e}")

    def _get_query_hash(self, query: str) -> str:
        """Genera hash √∫nico para una query"""
        return hashlib.md5(query.lower().encode()).hexdigest()

    def initialize(self) -> bool:
        """Inicializa sistema RAG m√©dico optimizado para velocidad"""
        try:
            logger.info("üöÄ Inicializando sistema RAG m√©dico R√ÅPIDO...")
            
            # 1. Sistema de recuperaci√≥n optimizado
            logger.info("üîç Cargando sistema de recuperaci√≥n...")
            self.retrieval_system = BM25DualChunkEvaluator(self.config_path, self.mode)
            self.retrieval_system.load_collection()
            
            # Optimizar par√°metros del sistema de recuperaci√≥n
            self._optimize_retrieval_system()
            logger.info("‚úÖ Sistema de recuperaci√≥n optimizado")
            
            # 2. Sistema de generaci√≥n r√°pido
            logger.info("ü§ñ Cargando Mistral optimizado...")
            self._initialize_fast_mistral()
            logger.info("‚úÖ Mistral optimizado cargado")
            
            self.is_initialized = True
            logger.info("‚úÖ Sistema RAG m√©dico R√ÅPIDO listo")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando sistema: {e}")
            self.is_initialized = False
            return False

    def _optimize_retrieval_system(self):
        """Optimiza par√°metros del sistema de recuperaci√≥n para velocidad"""
        if hasattr(self.retrieval_system, 'biencoder'):
            # Usar batch size mayor para embeddings
            self.retrieval_system.biencoder.encode_batch_size = 32
        
        logger.info("‚ö° Sistema de recuperaci√≥n optimizado para velocidad")

    def _initialize_fast_mistral(self):
        """Inicializa pipeline Mistral optimizado para velocidad"""
        device = 0 if torch.cuda.is_available() else -1
        
        try:
            # Usar par√°metros optimizados para velocidad
            self.mistral_pipeline = pipeline(
                "text-generation",
                model=self.mistral_model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device=device,
                trust_remote_code=True,
                # Optimizaciones para velocidad
                do_sample=True,
                pad_token_id=50256,  # GPT-style
                max_length=1024      # Limitar contexto
            )
            logger.info(f"‚úÖ Modelo r√°pido cargado: {self.mistral_model_name}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error con modelo local, usando fallback r√°pido: {e}")
            # Fallback a modelo m√°s peque√±o y r√°pido
            self.mistral_pipeline = pipeline(
                "text-generation",
                model="microsoft/DialoGPT-small",  # Modelo muy peque√±o
                torch_dtype=torch.float32,
                device=device,
                max_length=512
            )
        
        # Configurar pad token
        if self.mistral_pipeline.tokenizer.pad_token is None:
            self.mistral_pipeline.tokenizer.pad_token = self.mistral_pipeline.tokenizer.eos_token

    def fast_consult_doctor(self, medical_question: str) -> MedicalConsultation:
        """
        Consulta m√©dica R√ÅPIDA - Pipeline optimizado para velocidad
        
        Optimizaciones:
        - Pool reducido (10 chunks)
        - Cache de embeddings  
        - Early stopping
        - Menos tokens en generaci√≥n
        """
        import time
        start_time = time.time()
        
        if not self.is_initialized:
            return MedicalConsultation(
                question=medical_question,
                answer="‚ùå Sistema no inicializado.",
                best_chunk={},
                pipeline_stats={},
                success=False,
                processing_time=0.0
            )
        
        logger.info(f"‚ö° Consulta R√ÅPIDA: {medical_question[:50]}...")
        
        try:
            # ============ PIPELINE R√ÅPIDO ============
            
            # 1. Cache lookup
            query_hash = self._get_query_hash(medical_question)
            cached_result = self.query_cache.get(query_hash)
            
            if cached_result:
                logger.info("üì¶ Usando resultado cacheado")
                processing_time = time.time() - start_time
                cached_result.processing_time = processing_time
                return cached_result
            
            # 2. BM25 r√°pido (menos candidatos)
            logger.debug("1Ô∏è‚É£ BM25 r√°pido...")
            bm25_ranking = self.retrieval_system.calculate_bm25_rankings(medical_question)
            bm25_pool = bm25_ranking[:self.bm25_top_k]
            
            # 3. Bi-Encoder r√°pido (menos candidatos)
            logger.debug("2Ô∏è‚É£ Bi-Encoder r√°pido...")
            biencoder_ranking = self.retrieval_system.calculate_biencoder_rankings(medical_question)
            biencoder_pool = biencoder_ranking[:self.biencoder_top_k]
            
            # 4. Pool balanceado peque√±o
            logger.debug("3Ô∏è‚É£ Pool balanceado peque√±o...")
            balanced_pool = self.retrieval_system.create_balanced_chunk_pool(
                bm25_pool, biencoder_pool, pool_size=self.pool_size
            )
            
            # 5. Cross-Encoder con early stopping
            logger.debug("4Ô∏è‚É£ Cross-Encoder r√°pido...")
            final_ranking = self._fast_crossencoder_ranking(medical_question, balanced_pool)
            
            stats = {
                "bm25_candidates": len(bm25_pool),
                "biencoder_candidates": len(biencoder_pool),
                "balanced_pool_size": len(balanced_pool),
                "final_ranking_size": len(final_ranking),
                "cached": False
            }
            
            if not final_ranking:
                return MedicalConsultation(
                    question=medical_question,
                    answer="No encontr√© informaci√≥n relevante. Consulte con un m√©dico.",
                    best_chunk={},
                    pipeline_stats=stats,
                    success=False,
                    processing_time=time.time() - start_time
                )
            
            # 6. Preparar mejor chunk
            best_chunk_id = final_ranking[0]
            chunk_text = self.retrieval_system.docs_raw.get(best_chunk_id, '')
            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk_id, {})
            
            best_chunk_info = {
                "chunk_id": best_chunk_id,
                "text": chunk_text,
                "document_id": chunk_metadata.get('document_id', ''),
                "filename": chunk_metadata.get('filename', 'Gu√≠a m√©dica'),
                "chunk_position": chunk_metadata.get('chunk_position', ''),
                "categoria": chunk_metadata.get('categoria', 'medicina'),
                "text_length": len(chunk_text)
            }
            
            # 7. Generaci√≥n r√°pida
            logger.debug("5Ô∏è‚É£ Generando respuesta r√°pida...")
            medical_response = self._fast_generate_response(medical_question, best_chunk_info)
            
            # 8. Consulta completa
            consultation = MedicalConsultation(
                question=medical_question,
                answer=medical_response,
                best_chunk=best_chunk_info,
                pipeline_stats=stats,
                success=True,
                processing_time=time.time() - start_time
            )
            
            # Cache para consultas futuras (solo si es exitosa)
            self.query_cache[query_hash] = consultation
            if len(self.query_cache) % 10 == 0:  # Guardar cada 10 consultas
                self._save_cache()
            
            logger.info(f"‚úÖ Consulta r√°pida completada en {consultation.processing_time:.2f}s")
            return consultation
            
        except Exception as e:
            logger.error(f"‚ùå Error en consulta r√°pida: {e}")
            return MedicalConsultation(
                question=medical_question,
                answer=f"Error del sistema. Consulte con un m√©dico.",
                best_chunk={},
                pipeline_stats={},
                success=False,
                processing_time=time.time() - start_time
            )

    def _fast_crossencoder_ranking(self, query: str, chunk_pool: List[str]) -> List[str]:
        """Cross-Encoder optimizado con early stopping"""
        if not chunk_pool:
            return []
        
        # Limitar a√∫n m√°s el pool si es muy grande
        if len(chunk_pool) > 8:
            chunk_pool = chunk_pool[:8]
        
        try:
            # Preparar pares query-chunk
            query_chunk_pairs = []
            for chunk_id in chunk_pool:
                chunk_text = self.retrieval_system.docs_raw.get(chunk_id, '')
                if chunk_text:
                    # Truncar texto del chunk para velocidad
                    chunk_text_short = chunk_text[:400]  # M√°ximo 400 caracteres
                    query_chunk_pairs.append([query, chunk_text_short])
            
            if not query_chunk_pairs:
                return []
            
            # Scoring r√°pido
            scores = self.retrieval_system.cross_encoder.predict(query_chunk_pairs, batch_size=4)
            
            # Ordenar por score
            chunk_scores = list(zip(chunk_pool, scores))
            chunk_scores.sort(key=lambda x: x[1], reverse=True)
            
            return [chunk_id for chunk_id, _ in chunk_scores]
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error en cross-encoder r√°pido: {e}")
            return chunk_pool[:3]  # Fallback: devolver primeros 3

    def _fast_generate_response(self, question: str, best_chunk: Dict[str, Any]) -> str:
        """Generaci√≥n r√°pida con menos tokens"""
        
        filename = best_chunk.get('filename', 'Gu√≠a m√©dica')
        chunk_text = best_chunk.get('text', '')
        
        # Contexto m√°s corto
        medical_context = f"[{filename}]\n\n{chunk_text[:600]}"  # M√°ximo 600 chars
        
        # Prompt m√°s directo y corto
        prompt = (
            f"Como m√©dico, responde brevemente bas√°ndote en:\n{medical_context}\n\n"
            f"Pregunta: {question}\n\n"
            f"Respuesta m√©dica breve:"
        )
        
        try:
            # Generaci√≥n con par√°metros r√°pidos
            response = self.mistral_pipeline(
                prompt,
                max_new_tokens=self.max_new_tokens,      # 300 tokens
                temperature=self.temperature,            # 0.1 para menos exploraci√≥n  
                do_sample=True,
                repetition_penalty=self.repetition_penalty,
                pad_token_id=self.mistral_pipeline.tokenizer.eos_token_id,
                eos_token_id=self.mistral_pipeline.tokenizer.eos_token_id,
                truncation=True,
                # Par√°metros adicionales para velocidad
                top_p=0.9,  # Nucleus sampling m√°s restrictivo
                top_k=50    # Top-k sampling
            )
            
            generated = response[0]['generated_text']
            
            # Extraer respuesta
            if "Respuesta m√©dica breve:" in generated:
                answer = generated.split("Respuesta m√©dica breve:")[-1].strip()
            else:
                answer = generated[len(prompt):].strip()
            
            # Limpiar y limitar longitud
            answer = answer.replace("</s>", "").replace("<|endoftext|>", "").strip()
            
            # Asegurar respuesta no vac√≠a
            if not answer or len(answer) < 10:
                return self._emergency_fast_response(question, best_chunk)
            
            return answer
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error en generaci√≥n r√°pida: {e}")
            return self._emergency_fast_response(question, best_chunk)

    def _emergency_fast_response(self, question: str, best_chunk: Dict[str, Any]) -> str:
        """Respuesta de emergencia r√°pida"""
        filename = best_chunk.get('filename', 'documentaci√≥n m√©dica')
        
        return f"""Seg√∫n {filename}, su consulta sobre "{question}" requiere evaluaci√≥n m√©dica personalizada. 

RECOMENDACI√ìN: Consulte con su m√©dico de cabecera para evaluaci√≥n completa.

URGENCIAS: Si presenta s√≠ntomas graves, acuda inmediatamente a urgencias.

*[Respuesta autom√°tica r√°pida]*"""

    # ============ M√âTODOS DE UTILIDAD R√ÅPIDOS ============

    def clear_cache(self):
        """Limpia el cache de queries"""
        self.query_cache = {}
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)
        logger.info("üßπ Cache limpiado")

    def get_cache_stats(self) -> Dict[str, Any]:
        """Estad√≠sticas del cache"""
        return {
            "cached_queries": len(self.query_cache),
            "cache_file_exists": os.path.exists(self.cache_file),
            "cache_size_mb": os.path.getsize(self.cache_file) / (1024*1024) if os.path.exists(self.cache_file) else 0
        }

    def benchmark_speed(self, test_queries: List[str]) -> Dict[str, Any]:
        """Benchmark de velocidad del sistema"""
        if not self.is_initialized:
            return {"error": "Sistema no inicializado"}
        
        results = []
        total_time = 0
        
        logger.info(f"üèÉ‚Äç‚ôÇÔ∏è Iniciando benchmark con {len(test_queries)} queries...")
        
        for i, query in enumerate(test_queries, 1):
            logger.info(f"‚è±Ô∏è Query {i}/{len(test_queries)}: {query[:50]}...")
            
            consultation = self.fast_consult_doctor(query)
            results.append({
                "query": query[:100],
                "success": consultation.success,
                "processing_time": consultation.processing_time,
                "answer_length": len(consultation.answer),
                "cached": consultation.pipeline_stats.get("cached", False)
            })
            total_time += consultation.processing_time
            
            # Mostrar progreso
            avg_time = total_time / i
            logger.info(f"‚úÖ Completada en {consultation.processing_time:.2f}s (promedio: {avg_time:.2f}s)")
        
        # Estad√≠sticas finales
        times = [r["processing_time"] for r in results]
        successful = [r for r in results if r["success"]]
        
        benchmark_stats = {
            "total_queries": len(test_queries),
            "successful_queries": len(successful),
            "success_rate": len(successful) / len(test_queries) * 100,
            "total_time": total_time,
            "average_time": sum(times) / len(times),
            "min_time": min(times),
            "max_time": max(times),
            "cached_responses": len([r for r in results if r.get("cached", False)]),
            "detailed_results": results
        }
        
        logger.info(f"üèÅ Benchmark completado:")
        logger.info(f"   ‚è±Ô∏è Tiempo promedio: {benchmark_stats['average_time']:.2f}s")
        logger.info(f"   ‚úÖ Tasa de √©xito: {benchmark_stats['success_rate']:.1f}%")
        logger.info(f"   üì¶ Respuestas cacheadas: {benchmark_stats['cached_responses']}")
        
        return benchmark_stats


# ============ DEMOSTRACI√ìN R√ÅPIDA ============

def main():
    """Demostraci√≥n del RAG m√©dico optimizado para velocidad"""
    
    print("‚ö° RAG M√©dico FAST - Optimizado para Velocidad")
    print("="*60)
    print("Optimizaciones: Pool peque√±o + Cache + Tokens reducidos + Early stopping")
    print("="*60)
    
    # Configuraci√≥n
    config_path = "../config.yaml"
    
    # Inicializar sistema r√°pido
    fast_rag = FastMedicalRAG(config_path, mode="embedding")
    
    print("üöÄ Inicializando sistema r√°pido...")
    if not fast_rag.initialize():
        print("‚ùå Error en inicializaci√≥n")
        return
    
    # Consultas de prueba
    test_queries = [
        "Doctor, tengo mucha sed y orino frecuentemente. ¬øEs diabetes?",
        "Siento dolor en el pecho al hacer ejercicio. ¬øEs grave?", 
        "Estoy muy triste y sin energ√≠a √∫ltimamente. ¬øQu√© hago?",
        "Mi presi√≥n arterial sali√≥ alta. ¬øQu√© debo hacer?"
    ]
    
    # Benchmark de velocidad
    print(f"\n‚è±Ô∏è BENCHMARK DE VELOCIDAD")
    print("-" * 40)
    
    benchmark = fast_rag.benchmark_speed(test_queries)
    
    print(f"\nüìä RESULTADOS DEL BENCHMARK:")
    print(f"   üéØ Consultas exitosas: {benchmark['successful_queries']}/{benchmark['total_queries']}")
    print(f"   ‚è±Ô∏è Tiempo promedio: {benchmark['average_time']:.2f} segundos")
    print(f"   ‚ö° Tiempo m√≠nimo: {benchmark['min_time']:.2f} segundos")
    print(f"   üêå Tiempo m√°ximo: {benchmark['max_time']:.2f} segundos")
    print(f"   üì¶ Respuestas cacheadas: {benchmark['cached_responses']}")
    
    # Mostrar estad√≠sticas del cache
    cache_stats = fast_rag.get_cache_stats()
    print(f"\nüíæ ESTAD√çSTICAS DEL CACHE:")
    print(f"   üìù Queries cacheadas: {cache_stats['cached_queries']}")
    print(f"   üíΩ Tama√±o del cache: {cache_stats['cache_size_mb']:.2f} MB")
    
    # Demostraci√≥n de consulta individual r√°pida
    print(f"\nü©∫ CONSULTA INDIVIDUAL R√ÅPIDA")
    print("-" * 40)
    
    query_demo = "Doctor, tengo dolor de cabeza frecuente. ¬øQu√© puede ser?"
    print(f"üìù Consulta: {query_demo}")
    
    consultation = fast_rag.fast_consult_doctor(query_demo)
    
    if consultation.success:
        print(f"‚úÖ Respuesta en {consultation.processing_time:.2f} segundos")
        print(f"üìñ Fuente: {consultation.best_chunk['filename']}")
        print(f"\nüë®‚Äç‚öïÔ∏è RESPUESTA:")
        print(consultation.answer[:300] + "..." if len(consultation.answer) > 300 else consultation.answer)
    else:
        print(f"‚ùå Error: {consultation.answer}")
    
    print(f"\nüéâ Demostraci√≥n completada!")
    print(f"\nüí° Optimizaciones implementadas:")
    print(f"   üéØ Pool reducido a 10 chunks (vs 50 original)")
    print(f"   üì¶ Cache de consultas frecuentes")
    print(f"   ‚ö° Generaci√≥n con 300 tokens (vs 600 original)")
    print(f"   üõë Early stopping en cross-encoder")
    print(f"   üîß Par√°metros optimizados para velocidad")

if __name__ == "__main__":
    main()