"""
medical_rag_optimized_fast.py - RAG MÃ©dico Optimizado para VELOCIDAD

Optimizaciones principales:
1. Reduce pool_size de 50 a 10-15 chunks
2. Caching de embeddings frecuentes  
3. Early stopping en cross-encoder
4. Mistral con menos tokens
5. BM25 con menos candidatos
"""

import os
import sys
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
import pickle
import hashlib

# Imports para Mistral
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Importar la clase existente
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)

from retrieval.bm25_model_chunk_bge import BM25DualChunkEvaluator
from embeddings.load_model import cargar_configuracion

# Configurar logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

@dataclass
class MedicalConsultation:
    """Consulta mÃ©dica completa con respuesta"""
    question: str
    answer: str
    best_chunk: Dict[str, Any]
    pipeline_stats: Dict[str, int]
    success: bool
    processing_time: float = 0.0

class FastMedicalRAG:
    """
    RAG MÃ©dico Optimizado para VELOCIDAD
    
    Optimizaciones implementadas:
    - Pool reducido (10-15 chunks vs 50)
    - Cache de embeddings frecuentes
    - Early stopping en cross-encoder
    - Mistral con parÃ¡metros mÃ¡s rÃ¡pidos
    - BM25 limitado a top 10
    """
    
    def __init__(self, config_path: str, mode: str = "embedding"):
        """Inicializa RAG mÃ©dico optimizado para velocidad"""
        
        self.config_path = config_path
        
        # ========= PARÃMETROS ULTRA-OPTIMIZADOS PARA VELOCIDAD =========
        # OPCIÃ“N 1: Hardware lento (respuesta en ~2-3min)
        if mode == "ultra_fast":
            self.pool_size = 5           # MÃ­nimo viable
            self.bm25_top_k = 4          
            self.biencoder_top_k = 4      
            self.max_new_tokens = 150    # Respuestas muy cortas
            self.skip_cross_encoder = True  # Saltar cross-encoder
        else:
            # OPCIÃ“N 2: Balance velocidad-calidad (respuesta en ~3-4min)
            self.pool_size = 10           # Reducido de 50 a 10
            self.bm25_top_k = 8           # Reduci
        
        # Cache para embeddings frecuentes
        self.query_cache = {}
        self.cache_file = "query_embeddings_cache.pkl"
        self._load_cache()
        
        try:
            self.config = cargar_configuracion(config_path)
        except Exception:
            logger.warning("âš ï¸ No se pudo cargar config.yaml, usando valores por defecto.")
            self.config = {}

        self.mode = mode
        
        # Configurar modelos con parÃ¡metros rÃ¡pidos
        default_models_dir = os.path.join(PROJECT_ROOT, 'models')
        base_path = self.config.get('model_path', default_models_dir)
        llm = self.config.get('models', {}).get('llm_model', None)
        
        if llm:
            self.mistral_model_name = os.path.join(base_path, llm)
        else:
            # Modelo mÃ¡s pequeÃ±o como fallback
            self.mistral_model_name = "microsoft/DialoGPT-medium"
        
        # Componentes
        self.retrieval_system = None
        self.mistral_pipeline = None
        self.is_initialized = False

        logger.info("âš¡ RAG MÃ©dico FAST - Optimizado para velocidad")
        logger.info(f"ğŸ¯ Pool size: {self.pool_size}, BM25 top-k: {self.bm25_top_k}")

    def _load_cache(self):
        """Carga cache de embeddings de queries frecuentes"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'rb') as f:
                    self.query_cache = pickle.load(f)
                logger.info(f"ğŸ“¦ Cache cargado: {len(self.query_cache)} queries")
            else:
                self.query_cache = {}
        except Exception as e:
            logger.warning(f"âš ï¸ Error cargando cache: {e}")
            self.query_cache = {}

    def _save_cache(self):
        """Guarda cache de embeddings"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.query_cache, f)
        except Exception as e:
            logger.warning(f"âš ï¸ Error guardando cache: {e}")

    def _get_query_hash(self, query: str) -> str:
        """Genera hash Ãºnico para una query"""
        return hashlib.md5(query.lower().encode()).hexdigest()

    def initialize(self) -> bool:
        """Inicializa sistema RAG mÃ©dico optimizado para velocidad"""
        try:
            logger.info("ğŸš€ Inicializando sistema RAG mÃ©dico RÃPIDO...")
            
            # 1. Sistema de recuperaciÃ³n optimizado
            logger.info("ğŸ” Cargando sistema de recuperaciÃ³n...")
            self.retrieval_system = BM25DualChunkEvaluator(self.config_path, self.mode)
            self.retrieval_system.load_collection()
            
            # Optimizar parÃ¡metros del sistema de recuperaciÃ³n
            self._optimize_retrieval_system()
            logger.info("âœ… Sistema de recuperaciÃ³n optimizado")
            
            # 2. Sistema de generaciÃ³n rÃ¡pido
            logger.info("ğŸ¤– Cargando Mistral optimizado...")
            self._initialize_fast_mistral()
            logger.info("âœ… Mistral optimizado cargado")
            
            self.is_initialized = True
            logger.info("âœ… Sistema RAG mÃ©dico RÃPIDO listo")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error inicializando sistema: {e}")
            self.is_initialized = False
            return False

    def _optimize_retrieval_system(self):
        """Optimiza parÃ¡metros del sistema de recuperaciÃ³n para velocidad"""
        if hasattr(self.retrieval_system, 'biencoder'):
            # Usar batch size mayor para embeddings
            self.retrieval_system.biencoder.encode_batch_size = 32
        
        logger.info("âš¡ Sistema de recuperaciÃ³n optimizado para velocidad")

    def _initialize_fast_mistral(self):
        """Inicializa pipeline Mistral optimizado para velocidad"""
        device = 0 if torch.cuda.is_available() else -1
        
        try:
            # Usar parÃ¡metros optimizados para velocidad
            self.mistral_pipeline = pipeline(
                "text-generation",
                model=self.mistral_model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device=device,
                trust_remote_code=True,
                # Optimizaciones para velocidad
                do_sample=True,
                pad_token_id=50256,  # GPT-style
                max_length=1024      # Limitar contexto
            )
            logger.info(f"âœ… Modelo rÃ¡pido cargado: {self.mistral_model_name}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error con modelo local, usando fallback rÃ¡pido: {e}")
            # Fallback a modelo mÃ¡s pequeÃ±o y rÃ¡pido
            self.mistral_pipeline = pipeline(
                "text-generation",
                model="microsoft/DialoGPT-small",  # Modelo muy pequeÃ±o
                torch_dtype=torch.float32,
                device=device,
                max_length=512
            )
        
        # Configurar pad token
        if self.mistral_pipeline.tokenizer.pad_token is None:
            self.mistral_pipeline.tokenizer.pad_token = self.mistral_pipeline.tokenizer.eos_token

    def fast_consult_doctor(self, medical_question: str) -> MedicalConsultation:
        """
        Consulta mÃ©dica RÃPIDA - Pipeline optimizado para velocidad
        
        Optimizaciones:
        - Pool reducido (10 chunks)
        - Cache de embeddings  
        - Early stopping
        - Menos tokens en generaciÃ³n
        """
        import time
        start_time = time.time()
        
        if not self.is_initialized:
            return MedicalConsultation(
                question=medical_question,
                answer="âŒ Sistema no inicializado.",
                best_chunk={},
                pipeline_stats={},
                success=False,
                processing_time=0.0
            )
        
        logger.info(f"âš¡ Consulta RÃPIDA: {medical_question[:50]}...")
        
        try:
            # ============ PIPELINE RÃPIDO ============
            
            # 1. Cache lookup
            query_hash = self._get_query_hash(medical_question)
            cached_result = self.query_cache.get(query_hash)
            
            if cached_result:
                logger.info("ğŸ“¦ Usando resultado cacheado")
                processing_time = time.time() - start_time
                cached_result.processing_time = processing_time
                return cached_result
            
            # 2. BM25 rÃ¡pido (menos candidatos)
            logger.debug("1ï¸âƒ£ BM25 rÃ¡pido...")
            bm25_ranking = self.retrieval_system.calculate_bm25_rankings(medical_question)
            bm25_pool = bm25_ranking[:self.bm25_top_k]
            
            # 3. Bi-Encoder rÃ¡pido (menos candidatos)
            logger.debug("2ï¸âƒ£ Bi-Encoder rÃ¡pido...")
            biencoder_ranking = self.retrieval_system.calculate_biencoder_rankings(medical_question)
            biencoder_pool = biencoder_ranking[:self.biencoder_top_k]
            
            # 4. Pool balanceado pequeÃ±o
            logger.debug("3ï¸âƒ£ Pool balanceado pequeÃ±o...")
            balanced_pool = self.retrieval_system.create_balanced_chunk_pool(
                bm25_pool, biencoder_pool, pool_size=self.pool_size
            )
            
            # 5. Cross-Encoder con early stopping
            logger.debug("4ï¸âƒ£ Cross-Encoder rÃ¡pido...")
            final_ranking = self._fast_crossencoder_ranking(medical_question, balanced_pool)
            
            stats = {
                "bm25_candidates": len(bm25_pool),
                "biencoder_candidates": len(biencoder_pool),
                "balanced_pool_size": len(balanced_pool),
                "final_ranking_size": len(final_ranking),
                "cached": False
            }
            
            if not final_ranking:
                return MedicalConsultation(
                    question=medical_question,
                    answer="No encontrÃ© informaciÃ³n relevante. Consulte con un mÃ©dico.",
                    best_chunk={},
                    pipeline_stats=stats,
                    success=False,
                    processing_time=time.time() - start_time
                )
            
            # 6. Preparar mejor chunk
            best_chunk_id = final_ranking[0]
            chunk_text = self.retrieval_system.docs_raw.get(best_chunk_id, '')
            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk_id, {})
            
            best_chunk_info = {
                "chunk_id": best_chunk_id,
                "text": chunk_text,
                "document_id": chunk_metadata.get('document_id', ''),
                "filename": chunk_metadata.get('filename', 'GuÃ­a mÃ©dica'),
                "chunk_position": chunk_metadata.get('chunk_position', ''),
                "categoria": chunk_metadata.get('categoria', 'medicina'),
                "text_length": len(chunk_text)
            }
            
            # 7. GeneraciÃ³n rÃ¡pida
            logger.debug("5ï¸âƒ£ Generando respuesta rÃ¡pida...")
            medical_response = self._fast_generate_response(medical_question, best_chunk_info)
            
            # 8. Consulta completa
            consultation = MedicalConsultation(
                question=medical_question,
                answer=medical_response,
                best_chunk=best_chunk_info,
                pipeline_stats=stats,
                success=True,
                processing_time=time.time() - start_time
            )
            
            # Cache para consultas futuras (solo si es exitosa)
            self.query_cache[query_hash] = consultation
            if len(self.query_cache) % 10 == 0:  # Guardar cada 10 consultas
                self._save_cache()
            
            logger.info(f"âœ… Consulta rÃ¡pida completada en {consultation.processing_time:.2f}s")
            return consultation
            
        except Exception as e:
            logger.error(f"âŒ Error en consulta rÃ¡pida: {e}")
            return MedicalConsultation(
                question=medical_question,
                answer=f"Error del sistema. Consulte con un mÃ©dico.",
                best_chunk={},
                pipeline_stats={},
                success=False,
                processing_time=time.time() - start_time
            )

    def _fast_crossencoder_ranking(self, query: str, chunk_pool: List[str]) -> List[str]:
        """Cross-Encoder optimizado con early stopping"""
        if not chunk_pool:
            return []
        
        # Limitar aÃºn mÃ¡s el pool si es muy grande
        if len(chunk_pool) > 8:
            chunk_pool = chunk_pool[:8]
        
        try:
            # Preparar pares query-chunk
            query_chunk_pairs = []
            for chunk_id in chunk_pool:
                chunk_text = self.retrieval_system.docs_raw.get(chunk_id, '')
                if chunk_text:
                    # Truncar texto del chunk para velocidad
                    chunk_text_short = chunk_text[:400]  # MÃ¡ximo 400 caracteres
                    query_chunk_pairs.append([query, chunk_text_short])
            
            if not query_chunk_pairs:
                return []
            
            # Scoring rÃ¡pido
            scores = self.retrieval_system.cross_encoder.predict(query_chunk_pairs, batch_size=4)
            
            # Ordenar por score
            chunk_scores = list(zip(chunk_pool, scores))
            chunk_scores.sort(key=lambda x: x[1], reverse=True)
            
            return [chunk_id for chunk_id, _ in chunk_scores]
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error en cross-encoder rÃ¡pido: {e}")
            return chunk_pool[:3]  # Fallback: devolver primeros 3

    def _fast_generate_response(self, question: str, best_chunk: Dict[str, Any]) -> str:
        """GeneraciÃ³n rÃ¡pida con menos tokens"""
        
        filename = best_chunk.get('filename', 'GuÃ­a mÃ©dica')
        chunk_text = best_chunk.get('text', '')
        
        # Contexto mÃ¡s corto
        medical_context = f"[{filename}]\n\n{chunk_text[:600]}"  # MÃ¡ximo 600 chars
        
        # Prompt mÃ¡s directo y corto
        prompt = (
            f"Como mÃ©dico, responde brevemente basÃ¡ndote en:\n{medical_context}\n\n"
            f"Pregunta: {question}\n\n"
            f"Respuesta mÃ©dica breve:"
        )
        
        try:
            # GeneraciÃ³n con parÃ¡metros rÃ¡pidos
            response = self.mistral_pipeline(
                prompt,
                max_new_tokens=self.max_new_tokens,      # 300 tokens
                temperature=self.temperature,            # 0.1 para menos exploraciÃ³n  
                do_sample=True,
                repetition_penalty=self.repetition_penalty,
                pad_token_id=self.mistral_pipeline.tokenizer.eos_token_id,
                eos_token_id=self.mistral_pipeline.tokenizer.eos_token_id,
                truncation=True,
                # ParÃ¡metros adicionales para velocidad
                top_p=0.9,  # Nucleus sampling mÃ¡s restrictivo
                top_k=50    # Top-k sampling
            )
            
            generated = response[0]['generated_text']
            
            # Extraer respuesta
            if "Respuesta mÃ©dica breve:" in generated:
                answer = generated.split("Respuesta mÃ©dica breve:")[-1].strip()
            else:
                answer = generated[len(prompt):].strip()
            
            # Limpiar y limitar longitud
            answer = answer.replace("</s>", "").replace("<|endoftext|>", "").strip()
            
            # Asegurar respuesta no vacÃ­a
            if not answer or len(answer) < 10:
                return self._emergency_fast_response(question, best_chunk)
            
            return answer
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error en generaciÃ³n rÃ¡pida: {e}")
            return self._emergency_fast_response(question, best_chunk)

    def _emergency_fast_response(self, question: str, best_chunk: Dict[str, Any]) -> str:
        """Respuesta de emergencia rÃ¡pida"""
        filename = best_chunk.get('filename', 'documentaciÃ³n mÃ©dica')
        
        return f"""SegÃºn {filename}, su consulta sobre "{question}" requiere evaluaciÃ³n mÃ©dica personalizada. 

RECOMENDACIÃ“N: Consulte con su mÃ©dico de cabecera para evaluaciÃ³n completa.

URGENCIAS: Si presenta sÃ­ntomas graves, acuda inmediatamente a urgencias.

*[Respuesta automÃ¡tica rÃ¡pida]*"""

    # ============ MÃ‰TODOS DE UTILIDAD RÃPIDOS ============

    def clear_cache(self):
        """Limpia el cache de queries"""
        self.query_cache = {}
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)
        logger.info("ğŸ§¹ Cache limpiado")

    def get_cache_stats(self) -> Dict[str, Any]:
        """EstadÃ­sticas del cache"""
        return {
            "cached_queries": len(self.query_cache),
            "cache_file_exists": os.path.exists(self.cache_file),
            "cache_size_mb": os.path.getsize(self.cache_file) / (1024*1024) if os.path.exists(self.cache_file) else 0
        }

    def benchmark_speed(self, test_queries: List[str]) -> Dict[str, Any]:
        """Benchmark de velocidad del sistema"""
        if not self.is_initialized:
            return {"error": "Sistema no inicializado"}
        
        results = []
        total_time = 0
        
        logger.info(f"ğŸƒâ€â™‚ï¸ Iniciando benchmark con {len(test_queries)} queries...")
        
        for i, query in enumerate(test_queries, 1):
            logger.info(f"â±ï¸ Query {i}/{len(test_queries)}: {query[:50]}...")
            
            consultation = self.fast_consult_doctor(query)
            results.append({
                "query": query[:100],
                "success": consultation.success,
                "processing_time": consultation.processing_time,
                "answer_length": len(consultation.answer),
                "cached": consultation.pipeline_stats.get("cached", False)
            })
            total_time += consultation.processing_time
            
            # Mostrar progreso
            avg_time = total_time / i
            logger.info(f"âœ… Completada en {consultation.processing_time:.2f}s (promedio: {avg_time:.2f}s)")
        
        # EstadÃ­sticas finales
        times = [r["processing_time"] for r in results]
        successful = [r for r in results if r["success"]]
        
        benchmark_stats = {
            "total_queries": len(test_queries),
            "successful_queries": len(successful),
            "success_rate": len(successful) / len(test_queries) * 100,
            "total_time": total_time,
            "average_time": sum(times) / len(times),
            "min_time": min(times),
            "max_time": max(times),
            "cached_responses": len([r for r in results if r.get("cached", False)]),
            "detailed_results": results
        }
        
        logger.info(f"ğŸ Benchmark completado:")
        logger.info(f"   â±ï¸ Tiempo promedio: {benchmark_stats['average_time']:.2f}s")
        logger.info(f"   âœ… Tasa de Ã©xito: {benchmark_stats['success_rate']:.1f}%")
        logger.info(f"   ğŸ“¦ Respuestas cacheadas: {benchmark_stats['cached_responses']}")
        
        return benchmark_stats


# ============ DEMOSTRACIÃ“N RÃPIDA ============

def main():
    """DemostraciÃ³n del RAG mÃ©dico optimizado para velocidad"""
    
    print("âš¡ RAG MÃ©dico FAST - Optimizado para Velocidad")
    print("="*60)
    print("Optimizaciones: Pool pequeÃ±o + Cache + Tokens reducidos + Early stopping")
    print("="*60)
    
    # ConfiguraciÃ³n
    config_path = "../config.yaml"
    
    # Inicializar sistema rÃ¡pido
    fast_rag = FastMedicalRAG(config_path, mode="embedding")
    
    print("ğŸš€ Inicializando sistema rÃ¡pido...")
    if not fast_rag.initialize():
        print("âŒ Error en inicializaciÃ³n")
        return
    
    # Consultas de prueba
    test_queries = [
        "Doctor, tengo mucha sed y orino frecuentemente. Â¿Es diabetes?",
        "Siento dolor en el pecho al hacer ejercicio. Â¿Es grave?", 
        "Estoy muy triste y sin energÃ­a Ãºltimamente. Â¿QuÃ© hago?",
        "Mi presiÃ³n arterial saliÃ³ alta. Â¿QuÃ© debo hacer?"
    ]
    
    # Benchmark de velocidad
    print(f"\nâ±ï¸ BENCHMARK DE VELOCIDAD")
    print("-" * 40)
    
    benchmark = fast_rag.benchmark_speed(test_queries)
    
    print(f"\nğŸ“Š RESULTADOS DEL BENCHMARK:")
    print(f"   ğŸ¯ Consultas exitosas: {benchmark['successful_queries']}/{benchmark['total_queries']}")
    print(f"   â±ï¸ Tiempo promedio: {benchmark['average_time']:.2f} segundos")
    print(f"   âš¡ Tiempo mÃ­nimo: {benchmark['min_time']:.2f} segundos")
    print(f"   ğŸŒ Tiempo mÃ¡ximo: {benchmark['max_time']:.2f} segundos")
    print(f"   ğŸ“¦ Respuestas cacheadas: {benchmark['cached_responses']}")
    
    # Mostrar estadÃ­sticas del cache
    cache_stats = fast_rag.get_cache_stats()
    print(f"\nğŸ’¾ ESTADÃSTICAS DEL CACHE:")
    print(f"   ğŸ“ Queries cacheadas: {cache_stats['cached_queries']}")
    print(f"   ğŸ’½ TamaÃ±o del cache: {cache_stats['cache_size_mb']:.2f} MB")
    
    # DemostraciÃ³n de consulta individual rÃ¡pida
    print(f"\nğŸ©º CONSULTA INDIVIDUAL RÃPIDA")
    print("-" * 40)
    
    query_demo = "Doctor, tengo dolor de cabeza frecuente. Â¿QuÃ© puede ser?"
    print(f"ğŸ“ Consulta: {query_demo}")
    
    consultation = fast_rag.fast_consult_doctor(query_demo)
    
    if consultation.success:
        print(f"âœ… Respuesta en {consultation.processing_time:.2f} segundos")
        print(f"ğŸ“– Fuente: {consultation.best_chunk['filename']}")
        print(f"\nğŸ‘¨â€âš•ï¸ RESPUESTA:")
        print(consultation.answer[:300] + "..." if len(consultation.answer) > 300 else consultation.answer)
    else:
        print(f"âŒ Error: {consultation.answer}")
    
    print(f"\nğŸ‰ DemostraciÃ³n completada!")
    print(f"\nğŸ’¡ Optimizaciones implementadas:")
    print(f"   ğŸ¯ Pool reducido a 10 chunks (vs 50 original)")
    print(f"   ğŸ“¦ Cache de consultas frecuentes")
    print(f"   âš¡ GeneraciÃ³n con 300 tokens (vs 600 original)")
    print(f"   ğŸ›‘ Early stopping en cross-encoder")
    print(f"   ğŸ”§ ParÃ¡metros optimizados para velocidad")

if __name__ == "__main__":
    main()