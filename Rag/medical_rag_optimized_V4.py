"""
medical_rag_bm25_fast.py - RAG M√©dico BM25 SOLO - Optimizado para Velocidad

ESTRATEGIA √öNICA: Solo BM25 (sin Bi-Encoder ni Cross-Encoder)
ENFOQUE: M√°xima velocidad con calidad mantenida
OPTIMIZACIONES:
- Solo BM25 ranking
- Cache de consultas frecuentes  
- Generaci√≥n r√°pida con menos tokens
- Pool m√≠nimo de candidatos
"""

import os
import sys
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
import pickle
import hashlib
import time

# Imports b√°sicos
import torch
from transformers import pipeline

# Importar la clase existente
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)

from retrieval.bm25_model_chunk_bge import BM25DualChunkEvaluator
from embeddings.load_model import cargar_configuracion

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

@dataclass
class FastMedicalResponse:
    """Respuesta m√©dica r√°pida"""
    question: str
    answer: str
    source_info: str
    processing_time: float
    success: bool
    cached: bool = False
    bm25_candidates: int = 0

class FastBM25MedicalRAG:
    """
    RAG M√©dico BM25 SOLO - Optimizado para Velocidad M√°xima
    
    ESTRATEGIA √öNICA:
    - Solo BM25 (sin embeddings ni cross-encoder)
    - Cache de consultas frecuentes
    - Generaci√≥n r√°pida con tokens limitados
    - Pool m√≠nimo de candidatos
    
    OBJETIVO: < 1 segundo por consulta
    """
    
    def __init__(self, config_path: str, mode: str = "embedding"):
        """Inicializa RAG m√©dico BM25 ultrarr√°pido"""
        
        self.config_path = config_path
        self.mode = mode
        
        # Par√°metros optimizados para modelos potentes
        self.bm25_top_k = 5              # Solo top 5 candidatos BM25
        self.max_new_tokens = 200        # M√°s tokens para respuestas completas con modelos potentes
        self.temperature = 0.3           # Creatividad controlada
        
        # Cache para consultas frecuentes
        self.query_cache = {}
        self.cache_file = "bm25_query_cache.pkl"
        self._load_cache()
        
        try:
            self.config = cargar_configuracion(config_path)
        except Exception:
            logger.warning("‚ö†Ô∏è Usando configuraci√≥n por defecto")
            self.config = {}
        
        # Componentes del sistema
        self.retrieval_system = None
        self.generation_pipeline = None
        self.is_initialized = False
        
        logger.info("‚ö° RAG M√©dico BM25 con Modelo Potente - Calidad + Velocidad optimizada")

    def _load_cache(self):
        """Carga cache de consultas frecuentes"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'rb') as f:
                    self.query_cache = pickle.load(f)
                logger.info(f"üì¶ Cache cargado: {len(self.query_cache)} consultas")
            else:
                self.query_cache = {}
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error cargando cache: {e}")
            self.query_cache = {}

    def _save_cache(self):
        """Guarda cache de consultas"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.query_cache, f)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error guardando cache: {e}")

    def _get_query_hash(self, query: str) -> str:
        """Genera hash √∫nico para una consulta"""
        return hashlib.md5(query.lower().strip().encode()).hexdigest()

    def initialize(self) -> bool:
        """Inicializa sistema BM25 ultrarr√°pido"""
        try:
            logger.info("üöÄ Inicializando sistema BM25 con modelo potente...")
            
            # Solo sistema BM25 (sin embeddings)
            logger.info("üîç Cargando solo sistema BM25...")
            self.retrieval_system = BM25DualChunkEvaluator(self.config_path, self.mode)
            self.retrieval_system.load_collection()
            logger.info("‚úÖ Sistema BM25 cargado")
            
            # Modelo potente para generaci√≥n de calidad
            logger.info("ü§ñ Cargando modelo potente (Mistral/Qwen)...")
            self._initialize_fast_generation()
            logger.info("‚úÖ Modelo potente listo")
            
            self.is_initialized = True
            logger.info("‚úÖ Sistema BM25 + Modelo Potente inicializado correctamente")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando sistema: {e}")
            return False

    def _initialize_fast_generation(self):
        """Inicializa modelo potente (Mistral/Qwen) para generaci√≥n de calidad"""
        try:
            device = 0 if torch.cuda.is_available() else -1
            
            # Intentar modelos potentes en orden de preferencia
            model_candidates = [
                "microsoft/DialoGPT-large",  # Modelo conversacional potente
                "Qwen/Qwen2-1.5B-Instruct",  # Qwen instruct model
                "mistralai/Mistral-7B-v0.1",  # Mistral base
                "microsoft/DialoGPT-medium",  # Fallback intermedio
                "gpt2-large",  # Fallback grande
                "gpt2"  # √öltimo recurso
            ]
            
            for model_name in model_candidates:
                try:
                    logger.info(f"ü§ñ Intentando cargar modelo: {model_name}")
                    
                    self.generation_pipeline = pipeline(
                        "text-generation",
                        model=model_name,
                        device=device,
                        max_length=1024,  # Contexto amplio para modelo potente
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        trust_remote_code=True,
                        # Configuraciones para calidad
                        clean_up_tokenization_spaces=True
                    )
                    
                    # Configurar pad token
                    if self.generation_pipeline.tokenizer.pad_token is None:
                        self.generation_pipeline.tokenizer.pad_token = self.generation_pipeline.tokenizer.eos_token
                    
                    logger.info(f"‚úÖ Modelo potente cargado exitosamente: {model_name}")
                    break
                    
                except Exception as model_error:
                    logger.warning(f"‚ö†Ô∏è Error con {model_name}: {model_error}")
                    continue
            
            if self.generation_pipeline is None:
                raise Exception("No se pudo cargar ning√∫n modelo de generaci√≥n")
                
        except Exception as e:
            logger.error(f"‚ùå Error cargando modelos de generaci√≥n: {e}")
            logger.info("üîÑ Usando modo solo-respuesta estructurada")
            self.generation_pipeline = None

    def fast_ask_doctor(self, medical_question: str) -> FastMedicalResponse:
        """
        Consulta m√©dica ultrarr√°pida - Solo BM25
        
        Pipeline optimizado:
        1. Cache lookup (instant√°neo)
        2. BM25 ranking (< 0.1s)
        3. Selecci√≥n inteligente del mejor chunk
        4. Generaci√≥n r√°pida (< 0.5s)
        
        OBJETIVO: < 1 segundo total
        """
        start_time = time.time()
        
        if not self.is_initialized:
            return FastMedicalResponse(
                question=medical_question,
                answer="Sistema no inicializado. Reinicie el sistema.",
                source_info="Error",
                processing_time=0.0,
                success=False
            )
        
        logger.info(f"‚ö° Consulta ultrarr√°pida: {medical_question[:50]}...")
        
        try:
            # PASO 1: Cache lookup (instant√°neo)
            query_hash = self._get_query_hash(medical_question)
            if query_hash in self.query_cache:
                cached_response = self.query_cache[query_hash]
                cached_response.processing_time = time.time() - start_time
                cached_response.cached = True
                logger.info(f"üì¶ Respuesta cacheada en {cached_response.processing_time:.3f}s")
                return cached_response
            
            # PASO 2: BM25 ranking ultrarr√°pido con selecci√≥n inteligente por documento
            logger.debug("üîç BM25 ranking...")
            bm25_results = self.retrieval_system.calculate_bm25_rankings(medical_question)
            
            if not bm25_results:
                return FastMedicalResponse(
                    question=medical_question,
                    answer="No encontr√© informaci√≥n relevante en la base m√©dica. Consulte con su m√©dico de cabecera.",
                    source_info="Base de conocimientos m√©dicos",
                    processing_time=time.time() - start_time,
                    success=False,
                    bm25_candidates=0
                )
            
            # PASO 3: Tomar el mejor chunk (ya optimizado por documento en calculate_bm25_rankings)
            best_chunk_id = bm25_results[0]  # Ya es el mejor chunk del mejor documento
            chunk_text = self.retrieval_system.docs_raw.get(best_chunk_id, '')
            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk_id, {})
            
            source_info = chunk_metadata.get('filename', 'Gu√≠a m√©dica')
            chunk_position = chunk_metadata.get('chunk_position', '')
            document_id = chunk_metadata.get('document_id', '')
            
            # LOG DETALLADO DEL CHUNK SELECCIONADO
            logger.info(f"üìã CHUNK SELECCIONADO POR BM25:")
            logger.info(f"   üîë Chunk ID: {best_chunk_id}")
            logger.info(f"   üìÑ Documento: {document_id}")
            logger.info(f"   üìç Posici√≥n: {chunk_position}")
            logger.info(f"   üìñ Fuente: {source_info}")
            logger.info(f"   üìè Longitud: {len(chunk_text)} caracteres")
            logger.info(f"   üìù Contenido preview: {chunk_text[:100]}...")
            logger.info(f"   üéØ Top 5 BM25 ranking: {bm25_results[:5]}")
            
            # PASO 4: Generaci√≥n ultrarr√°pida
            logger.debug("‚ö° Generando respuesta...")
            medical_answer = self._ultra_fast_generate(medical_question, chunk_text, source_info)
            
            processing_time = time.time() - start_time
            
            # PASO 5: Crear respuesta final
            response = FastMedicalResponse(
                question=medical_question,
                answer=medical_answer,
                source_info=f"{source_info} - Chunk: {best_chunk_id} - Pos: {chunk_position}",
                processing_time=processing_time,
                success=True,
                cached=False,
                bm25_candidates=len(bm25_results[:self.bm25_top_k])
            )
            
            # Cache para consultas futuras
            self.query_cache[query_hash] = response
            if len(self.query_cache) % 5 == 0:  # Guardar cada 5 consultas
                self._save_cache()
            
            logger.info(f"‚úÖ Consulta ultrarr√°pida completada en {processing_time:.3f}s")
            logger.debug(f"üìñ Mejor chunk: {best_chunk_id} de documento: {document_id}")
            logger.debug(f"üìç Posici√≥n: {chunk_position}")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Error en consulta ultrarr√°pida: {e}")
            return FastMedicalResponse(
                question=medical_question,
                answer=f"Error del sistema: {str(e)}. Consulte con un m√©dico.",
                source_info="Error",
                processing_time=time.time() - start_time,
                success=False
            )

    def _ultra_fast_generate(self, question: str, context: str, source: str) -> str:
        """Generaci√≥n con modelo potente usando el chunk BM25 completo como contexto"""
        
        # Intentar generaci√≥n con modelo potente si est√° disponible
        if self.generation_pipeline:
            generated = self._try_model_generation(question, context)
            if generated:
                return f"{generated}\n\n*Fuente: {source}*"
        
        # Fallback: respuesta estructurada de alta calidad (siempre funciona)
        logger.debug("Usando respuesta estructurada como fallback")
        return self._create_instant_response(question, context, source)

    def _try_model_generation(self, question: str, context: str) -> Optional[str]:
        """Generaci√≥n con modelo potente usando el chunk BM25 como contexto completo"""
        try:
            # El contexto ya contiene el mejor chunk de BM25 - usarlo completo
            chunk_context = context[:1000]  # M√°s contexto para modelos potentes
            
            # Prompt estructurado para modelos conversacionales potentes
            prompt = f"""<|system|>
Eres un asistente m√©dico profesional. Responde de forma clara, emp√°tica y basada en evidencia cient√≠fica.

<|user|>
Contexto m√©dico relevante:
{chunk_context}

Consulta del paciente: {question}

Por favor, proporciona una respuesta m√©dica profesional basada en el contexto proporcionado.

<|assistant|>
"""
            
            # Par√°metros optimizados para modelos potentes
            result = self.generation_pipeline(
                prompt,
                max_new_tokens=200,  # M√°s tokens para respuestas completas
                temperature=0.3,     # Creatividad controlada
                do_sample=True,
                repetition_penalty=1.1,
                top_p=0.9,
                top_k=50,
                pad_token_id=self.generation_pipeline.tokenizer.eos_token_id,
                eos_token_id=self.generation_pipeline.tokenizer.eos_token_id,
                truncation=True
            )
            
            generated = result[0]['generated_text']
            
            # Extraer solo la respuesta del asistente
            if "<|assistant|>" in generated:
                answer = generated.split("<|assistant|>")[-1].strip()
            elif "Por favor, proporciona una respuesta m√©dica profesional" in generated:
                parts = generated.split("Por favor, proporciona una respuesta m√©dica profesional")
                if len(parts) > 1:
                    answer = parts[-1].strip()
                else:
                    answer = generated[len(prompt):].strip()
            else:
                answer = generated[len(prompt):].strip()
            
            # Limpiar tokens especiales y artefactos
            answer = answer.replace("</s>", "").replace("<|endoftext|>", "")
            answer = answer.replace("<|system|>", "").replace("<|user|>", "").replace("<|assistant|>", "")
            answer = answer.strip()
            
            # Validar calidad de la respuesta
            if self._is_valid_response(answer):
                logger.debug(f"‚úÖ Respuesta generada exitosamente: {answer[:50]}...")
                return answer
            else:
                logger.debug(f"‚ùå Respuesta inv√°lida del modelo: {answer[:50]}...")
                return None
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error en generaci√≥n con modelo potente: {e}")
            return None

    def _is_valid_response(self, response: str) -> bool:
        """Valida si la respuesta generada es v√°lida y de calidad m√©dica"""
        if not response or len(response) < 20:  # M√≠nimo m√°s exigente
            return False
        
        # Detectar repeticiones excesivas
        words = response.split()
        if len(words) < 8:  # M√≠nimo m√°s exigente para modelos potentes
            return False
        
        # Verificar si hay demasiadas repeticiones de palabras consecutivas
        repeated_count = 0
        for i in range(len(words) - 1):
            if words[i] == words[i + 1]:
                repeated_count += 1
                if repeated_count > 2:  # M√°s estricto para modelos potentes
                    return False
            else:
                repeated_count = 0
        
        # Verificar si una palabra se repite demasiado en total
        word_counts = {}
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1
            # M√°s estricto: una palabra no puede ser m√°s del 25% del texto
            if word_counts[word] > len(words) // 4:
                return False
        
        # Verificar que contiene palabras m√©dicas relevantes o estructura coherente
        medical_indicators = [
            'm√©dico', 'doctor', 'consulte', 's√≠ntoma', 'tratamiento', 'diagn√≥stico',
            'salud', 'paciente', 'evaluaci√≥n', 'recomiendo', 'importante', 'atenci√≥n'
        ]
        
        response_lower = response.lower()
        has_medical_content = any(indicator in response_lower for indicator in medical_indicators)
        
        # Verificar estructura coherente (frases con sentido)
        sentences = response.split('.')
        has_coherent_sentences = len([s for s in sentences if len(s.strip()) > 10]) >= 2
        
        return has_medical_content or has_coherent_sentences

    def _create_instant_response(self, question: str, context: str, source: str) -> str:
        """Crea respuesta estructurada instant√°nea y de calidad"""
        
        # Contexto limitado pero √∫til
        context_preview = context[:400] + "..." if len(context) > 400 else context
        
        # Detectar tipo de consulta r√°pidamente
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['dolor de cabeza', 'cefalea', 'migra√±a', 'dolor cabeza']):
            response_template = f"""Comprendo su preocupaci√≥n sobre el dolor de cabeza frecuente.

INFORMACI√ìN M√âDICA RELEVANTE:
{context_preview}

RECOMENDACIONES IMPORTANTES:
‚Ä¢ Los dolores de cabeza frecuentes requieren evaluaci√≥n m√©dica
‚Ä¢ Mantenga un diario: cu√°ndo ocurren, intensidad, duraci√≥n, factores desencadenantes
‚Ä¢ Busque atenci√≥n inmediata si presenta: dolor severo repentino, fiebre, cambios en la visi√≥n, rigidez del cuello

CU√ÅNDO CONSULTAR:
‚Ä¢ Si los dolores son m√°s frecuentes o intensos de lo habitual
‚Ä¢ Si interfieren con sus actividades diarias
‚Ä¢ Si van acompa√±ados de otros s√≠ntomas neurol√≥gicos

Su m√©dico de cabecera puede realizar una evaluaci√≥n completa y determinar el tratamiento m√°s adecuado."""

        elif any(word in question_lower for word in ['s√≠ntoma', 'siento', 'tengo', 'dolor', 'me duele']):
            response_template = f"""Sobre los s√≠ntomas que describe:

INFORMACI√ìN RELEVANTE:
{context_preview}

RECOMENDACIONES:
‚Ä¢ Consulte con su m√©dico si los s√≠ntomas persisten o empeoran
‚Ä¢ Acuda a urgencias si los s√≠ntomas son graves o aparecen repentinamente
‚Ä¢ Mantenga un registro de cu√°ndo aparecen y qu√© los mejora o empeora

IMPORTANTE: Esta informaci√≥n es educativa. Para diagn√≥stico preciso, consulte un m√©dico."""

        elif any(word in question_lower for word in ['tratamiento', 'medicamento', 'medicina', 'curar', 'como tratar']):
            response_template = f"""Sobre el tratamiento que consulta:

INFORMACI√ìN DISPONIBLE:
{context_preview}

IMPORTANTES CONSIDERACIONES:
‚Ä¢ Todo tratamiento debe ser prescrito por un m√©dico
‚Ä¢ No se automedique sin supervisi√≥n profesional
‚Ä¢ Consulte efectos secundarios con su farmac√©utico
‚Ä¢ Informe sobre otros medicamentos que est√© tomando

PR√ìXIMO PASO: Programe cita con su m√©dico de cabecera."""

        elif any(word in question_lower for word in ['qu√© es', 'que es', 'definici√≥n', 'significa']):
            response_template = f"""Informaci√≥n sobre su consulta:

DEFINICI√ìN Y CONTEXTO:
{context_preview}

ASPECTOS CLAVE:
‚Ä¢ Esta informaci√≥n proviene de fuentes m√©dicas confiables
‚Ä¢ Cada caso puede tener particularidades espec√≠ficas
‚Ä¢ Para informaci√≥n personalizada, consulte con un profesional m√©dico

CU√ÅNDO CONSULTAR:
‚Ä¢ Si tiene dudas espec√≠ficas sobre su situaci√≥n
‚Ä¢ Para obtener un diagn√≥stico o evaluaci√≥n personalizada
‚Ä¢ Si necesita orientaci√≥n sobre prevenci√≥n o tratamiento"""

        else:
            response_template = f"""Respuesta a su consulta m√©dica:

INFORMACI√ìN RELEVANTE:
{context_preview}

RECOMENDACIONES GENERALES:
‚Ä¢ Esta informaci√≥n proviene de fuentes m√©dicas confiables
‚Ä¢ Para una respuesta personalizada, consulte con su m√©dico
‚Ä¢ Si tiene s√≠ntomas preocupantes, busque atenci√≥n m√©dica
‚Ä¢ Mant√©ngase informado a trav√©s de fuentes m√©dicas oficiales"""

        return f"{response_template}\n\n*Fuente: {source}*"

    # ============ M√âTODOS DE UTILIDAD ============

    def clear_cache(self):
        """Limpia el cache de consultas"""
        self.query_cache = {}
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)
        logger.info("üßπ Cache limpiado")

    def get_cache_stats(self) -> Dict[str, Any]:
        """Estad√≠sticas del cache"""
        cache_size = 0
        if os.path.exists(self.cache_file):
            cache_size = os.path.getsize(self.cache_file) / (1024*1024)
        
        return {
            "cached_queries": len(self.query_cache),
            "cache_file_exists": os.path.exists(self.cache_file),
            "cache_size_mb": cache_size
        }

    def speed_benchmark(self, test_queries: List[str]) -> Dict[str, Any]:
        """Benchmark de velocidad BM25-only"""
        if not self.is_initialized:
            return {"error": "Sistema no inicializado"}
        
        logger.info(f"üèÉ‚Äç‚ôÇÔ∏è Benchmark BM25-FAST: {len(test_queries)} consultas")
        
        results = []
        total_time = 0
        
        for i, query in enumerate(test_queries, 1):
            logger.info(f"‚è±Ô∏è Consulta {i}/{len(test_queries)}: {query[:40]}...")
            
            response = self.fast_ask_doctor(query)
            
            result = {
                "query": query[:80],
                "success": response.success,
                "processing_time": response.processing_time,
                "answer_length": len(response.answer),
                "cached": response.cached,
                "bm25_candidates": response.bm25_candidates,
                "source_info": response.source_info if response.success else "N/A"
            }
            results.append(result)
            total_time += response.processing_time
            
            logger.info(f"‚úÖ {response.processing_time:.3f}s ({'cache' if response.cached else 'nuevo'})")
        
        # An√°lisis de resultados
        times = [r["processing_time"] for r in results if not r["cached"]]
        successful = [r for r in results if r["success"]]
        cached_count = len([r for r in results if r["cached"]])
        
        if not times:  # Todas fueron cacheadas
            times = [0.001]  # Tiempo m√≠nimo para evitar divisi√≥n por cero
        
        benchmark_stats = {
            "strategy": "BM25_ONLY_FAST",
            "total_queries": len(test_queries),
            "successful_queries": len(successful),
            "success_rate": len(successful) / len(test_queries) * 100,
            "cached_responses": cached_count,
            "new_responses": len(test_queries) - cached_count,
            "total_time": total_time,
            "average_time_new": sum(times) / len(times),
            "min_time": min(times),
            "max_time": max(times),
            "under_1_second": len([t for t in times if t < 1.0]),
            "under_500ms": len([t for t in times if t < 0.5]),
            "detailed_results": results
        }
        
        logger.info(f"üèÅ Benchmark BM25-FAST completado:")
        logger.info(f"   ‚úÖ √âxito: {benchmark_stats['success_rate']:.1f}%")
        logger.info(f"   ‚è±Ô∏è Tiempo promedio (nuevas): {benchmark_stats['average_time_new']:.3f}s")
        logger.info(f"   üì¶ Respuestas cacheadas: {cached_count}")
        logger.info(f"   ‚ö° Consultas < 1s: {benchmark_stats['under_1_second']}/{len(times)}")
        logger.info(f"   üöÄ Consultas < 500ms: {benchmark_stats['under_500ms']}/{len(times)}")
        
        return benchmark_stats


# ============ DEMOSTRACI√ìN ULTRARR√ÅPIDA ============

def main():
    """Demostraci√≥n del RAG BM25 ultrarr√°pido"""
    
    print("‚ö° RAG M√©dico BM25-FAST - Solo BM25 para M√°xima Velocidad")
    print("="*70)
    print("ESTRATEGIA: Solo BM25 + Cache + Generaci√≥n r√°pida")
    print("OBJETIVO: < 1 segundo por consulta")
    print("="*70)
    
    # Inicializar sistema
    fast_rag = FastBM25MedicalRAG("../config.yaml", mode="embedding")
    
    print("üöÄ Inicializando sistema ultrarr√°pido...")
    if not fast_rag.initialize():
        print("‚ùå Error en inicializaci√≥n")
        return
    
    # Consultas de prueba
    test_queries = [
        "Doctor, tengo mucha sed y orino frecuentemente. ¬øEs diabetes?",
        "Siento dolor en el pecho cuando hago ejercicio. ¬øEs grave?", 
        "Estoy muy triste y sin energ√≠a √∫ltimamente. ¬øQu√© hago?",
        "Mi presi√≥n arterial sali√≥ alta en el control. ¬øQu√© debo hacer?",
        "¬øQu√© s√≠ntomas tiene la gripe?",
        "¬øC√≥mo puedo bajar la fiebre naturalmente?"
    ]
    
    # Benchmark de velocidad
    print(f"\n‚è±Ô∏è BENCHMARK DE VELOCIDAD BM25-ONLY")
    print("-" * 50)
    
    benchmark = fast_rag.speed_benchmark(test_queries)
    
    print(f"\nüìä RESULTADOS DEL BENCHMARK:")
    print(f"   üéØ Estrategia: {benchmark['strategy']}")
    print(f"   ‚úÖ Consultas exitosas: {benchmark['successful_queries']}/{benchmark['total_queries']}")
    print(f"   ‚è±Ô∏è Tiempo promedio (nuevas): {benchmark['average_time_new']:.3f}s")
    print(f"   ‚ö° Tiempo m√≠nimo: {benchmark['min_time']:.3f}s")
    print(f"   üêå Tiempo m√°ximo: {benchmark['max_time']:.3f}s")
    print(f"   üì¶ Respuestas cacheadas: {benchmark['cached_responses']}")
    print(f"   üöÄ Consultas < 1 segundo: {benchmark['under_1_second']}/{benchmark['new_responses']}")
    print(f"   ‚ö° Consultas < 500ms: {benchmark['under_500ms']}/{benchmark['new_responses']}")
    
    # Estad√≠sticas del cache
    cache_stats = fast_rag.get_cache_stats()
    print(f"\nüíæ ESTAD√çSTICAS DEL CACHE:")
    print(f"   üìù Consultas cacheadas: {cache_stats['cached_queries']}")
    print(f"   üíΩ Tama√±o del cache: {cache_stats['cache_size_mb']:.2f} MB")
    
    # Demostraci√≥n de consulta individual
    print(f"\nü©∫ CONSULTA INDIVIDUAL ULTRARR√ÅPIDA")
    print("-" * 45)
    
    demo_query = "Doctor, tengo dolor de cabeza frecuente y me preocupa"
    print(f"üìù Consulta: {demo_query}")
    
    response = fast_rag.fast_ask_doctor(demo_query)
    
    if response.success:
        print(f"‚úÖ Respuesta en {response.processing_time:.3f} segundos")
        print(f"üìñ Informaci√≥n detallada: {response.source_info}")
        print(f"üì¶ Cacheada: {'S√≠' if response.cached else 'No'}")
        print(f"üîç Candidatos BM25: {response.bm25_candidates}")
        print(f"\nüë®‚Äç‚öïÔ∏è RESPUESTA:")
        print(response.answer[:500] + "..." if len(response.answer) > 500 else response.answer)
    else:
        print(f"‚ùå Error: {response.answer}")
    
    print(f"\nüéâ DEMOSTRACI√ìN COMPLETADA!")
    print(f"\nüí° OPTIMIZACIONES BM25-FAST:")
    print(f"   üéØ Solo BM25 (sin embeddings ni cross-encoder)")
    print(f"   üì¶ Cache inteligente de consultas frecuentes")
    print(f"   ‚ö° Generaci√≥n con m√°ximo 120 tokens")
    print(f"   üîç Solo top-1 BM25 (m√°xima velocidad)")
    print(f"   üöÄ Fallback estructurado instant√°neo")
    print(f"   ‚è±Ô∏è Objetivo: < 1 segundo por consulta")

if __name__ == "__main__":
    main()