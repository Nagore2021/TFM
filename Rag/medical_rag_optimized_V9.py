"""
medical_rag_optimized_finetuned.py - RAG M√©dico Optimizado con Modelo Fine-tuneado

OBJETIVO: Mejor rendimiento usando modelo fine-tuneado m√©dico
PROCESO: Pregunta ‚Üí Pipeline H√≠brido Optimizado ‚Üí Mejor Chunk ‚Üí Respuesta M√©dica
OPTIMIZACIONES: Modelo fine-tuneado + Pool reducido + Transparencia mejorada
"""

import os
import sys
import logging
from typing import Dict, Any, Optional
from dataclasses import dataclass
import time

# Imports b√°sicos
import torch
from transformers import pipeline

# Importar la clase existente
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)

from retrieval.bm25_model_chunk_bge import BM25DualChunkEvaluator
from embeddings.load_model import cargar_configuracion

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

@dataclass
class OptimizedMedicalResponse:
    """Respuesta m√©dica optimizada"""
    question: str
    answer: str
    chunk_used: Dict[str, Any]
    processing_time: float
    success: bool

class OptimizedMedicalRAG:
    """
    RAG M√©dico Optimizado con Modelo Fine-tuneado
    
    OPTIMIZACIONES IMPLEMENTADAS:
    1. Modelo fine-tuneado m√©dico (m√°s r√°pido y preciso)
    2. Pipeline h√≠brido con pool reducido (velocidad)
    3. Transparencia mejorada (no se hace pasar por m√©dico)
    4. Respuestas estructuradas profesionales
    """
    
    def __init__(self, config_path: str, mode: str = "finetuneado"):
        """Inicializa RAG m√©dico optimizado con modelo fine-tuneado"""
        
        self.config_path = config_path
        self.mode = mode
        
        try:
            self.config = cargar_configuracion(config_path)
        except Exception:
            logger.warning("‚ö†Ô∏è Usando configuraci√≥n por defecto")
            self.config = {}
        
        # Componentes del sistema
        self.retrieval_system = None
        self.generation_pipeline = None
        self.is_initialized = False
        
        logger.info("üéØ RAG M√©dico Optimizado con Modelo Fine-tuneado - Velocidad + Precisi√≥n")

    def initialize(self) -> bool:
        """Inicializa el sistema optimizado"""
        try:
            print("\nüîß INICIALIZANDO SISTEMA OPTIMIZADO...")
            
            # 1. Cargar sistema con modelo fine-tuneado
            print("üìö Cargando base de conocimientos m√©dicos (modelo fine-tuneado)...")
            self.retrieval_system = BM25DualChunkEvaluator(self.config_path, self.mode)
            self.retrieval_system.load_collection()
            
            total_chunks = len(self.retrieval_system.chunk_ids)
            print(f"‚úÖ Base cargada: {total_chunks} fragmentos m√©dicos disponibles")
            print(f"üéØ Modelo: {self.mode} (optimizado para consultas m√©dicas)")
            
            # 2. No cargar modelo de generaci√≥n - solo respuestas estructuradas
            print("üí° Usando respuestas estructuradas profesionales (m√°s confiables)")
            self.generation_pipeline = None
            
            self.is_initialized = True
            print("‚úÖ Sistema optimizado listo para consultas m√©dicas\n")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error inicializando: {e}")
            return False

    def ask_doctor(self, medical_question: str) -> OptimizedMedicalResponse:
        """
        Consulta m√©dica optimizada - Proceso transparente y eficiente
        """
        start_time = time.time()
        
        if not self.is_initialized:
            return OptimizedMedicalResponse(
                question=medical_question,
                answer="Sistema no inicializado",
                chunk_used={},
                processing_time=0.0,
                success=False
            )
        
        print(f"\nüí¨ CONSULTA M√âDICA: {medical_question}")
        print("="*60)
        
        try:
            # PASO 1: Buscar informaci√≥n m√©dica relevante
            print("üîç PASO 1: Buscando informaci√≥n m√©dica relevante...")
            chunk_info = self._find_best_medical_chunk_optimized(medical_question)
            
            if not chunk_info:
                return OptimizedMedicalResponse(
                    question=medical_question,
                    answer="No encontr√© informaci√≥n m√©dica relevante para su consulta. Le recomiendo consultar con un profesional m√©dico.",
                    chunk_used={},
                    processing_time=time.time() - start_time,
                    success=False
                )
            
            # PASO 2: Mostrar informaci√≥n encontrada
            self._show_chunk_info_detailed(chunk_info)
            
            # PASO 3: Generar respuesta m√©dica profesional
            print("\nü§ñ PASO 3: Generando respuesta m√©dica profesional...")
            medical_answer = self._generate_professional_answer(medical_question, chunk_info)
            
            processing_time = time.time() - start_time
            
            response = OptimizedMedicalResponse(
                question=medical_question,
                answer=medical_answer,
                chunk_used=chunk_info,
                processing_time=processing_time,
                success=True
            )
            
            # PASO 4: Mostrar respuesta final
            self._show_final_response(response)
            
            return response
            
        except Exception as e:
            print(f"‚ùå ERROR: {e}")
            return OptimizedMedicalResponse(
                question=medical_question,
                answer=f"Error del sistema: {str(e)}",
                chunk_used={},
                processing_time=time.time() - start_time,
                success=False
            )

    def _find_best_medical_chunk_optimized(self, question: str) -> Optional[Dict[str, Any]]:
        """Busca el mejor fragmento m√©dico usando m√©todos preparados de BM25DualChunkEvaluator"""
        try:
            print("üéØ Usando m√©todos de la clase BM25DualChunkEvaluator preparada...")
            
            # OPCI√ìN 1: Usar m√©todo h√≠brido preparado (el que da mejor calidad)
            print("üîç Ejecutando calculate_hybrid_pipeline...")
            hybrid_results = self.retrieval_system.calculate_hybrid_pipeline(
                query=question, 
                pool_size=6,     # Pool reducido para velocidad
                batch_size=4     # Batch peque√±o para eficiencia
            )
            
            if hybrid_results:
                best_chunk = hybrid_results[0]
                strategy_name = "H√≠brido Preparado (BM25+Bi-Encoder+Cross-Encoder)"
                print(f"‚úÖ H√≠brido exitoso: {best_chunk}")
            else:
                # OPCI√ìN 2: Fallback a BM25 preparado
                print("‚ö†Ô∏è H√≠brido vac√≠o, usando calculate_bm25_rankings...")
                bm25_results = self.retrieval_system.calculate_bm25_rankings(question)
                if bm25_results:
                    best_chunk = bm25_results[0]
                    strategy_name = "BM25 Preparado"
                    hybrid_results = bm25_results
                    print(f"‚úÖ BM25 exitoso: {best_chunk}")
                else:
                    return None
            
            # Obtener informaci√≥n del chunk usando m√©todos preparados
            chunk_text = self.retrieval_system.docs_raw.get(best_chunk, '')
            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk, {})
            
            return {
                "chunk_id": best_chunk,
                "text": chunk_text,
                "filename": chunk_metadata.get('filename', 'Gu√≠a m√©dica'),
                "document_id": chunk_metadata.get('document_id', ''),
                "chunk_position": chunk_metadata.get('chunk_position', ''),
                "categoria": chunk_metadata.get('categoria', 'medicina'),
                "strategy_used": f"{strategy_name} (modelo {self.mode})",
                "pool_size": 6,
                "model_type": self.mode,
                "all_results": hybrid_results[:5],  # Top 5 para mostrar
                "class_methods": "BM25DualChunkEvaluator preparada"
            }
            
        except Exception as e:
            print(f"‚ùå Error usando m√©todos preparados: {e}")
            print("üîÑ Fallback a m√©todos b√°sicos...")
            return self._find_best_medical_chunk_basic_fallback(question)

    def _find_best_medical_chunk_basic_fallback(self, question: str) -> Optional[Dict[str, Any]]:
        """Fallback b√°sico si fallan los m√©todos preparados"""
        try:
            print("üîÑ Usando m√©todos b√°sicos como fallback...")
            
            # Intentar m√©todos individuales preparados
            methods_to_try = [
                ("calculate_bm25_rankings", "BM25 Individual"),
                ("calculate_biencoder_rankings", "Bi-Encoder Individual"),
            ]
            
            for method_name, strategy_name in methods_to_try:
                try:
                    if hasattr(self.retrieval_system, method_name):
                        print(f"üîç Probando {method_name}...")
                        method = getattr(self.retrieval_system, method_name)
                        results = method(question)
                        
                        if results:
                            best_chunk = results[0]
                            chunk_text = self.retrieval_system.docs_raw.get(best_chunk, '')
                            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk, {})
                            
                            print(f"‚úÖ {method_name} exitoso: {best_chunk}")
                            
                            return {
                                "chunk_id": best_chunk,
                                "text": chunk_text,
                                "filename": chunk_metadata.get('filename', 'Gu√≠a m√©dica'),
                                "document_id": chunk_metadata.get('document_id', ''),
                                "chunk_position": chunk_metadata.get('chunk_position', ''),
                                "categoria": chunk_metadata.get('categoria', 'medicina'),
                                "strategy_used": f"{strategy_name} Fallback (modelo {self.mode})",
                                "model_type": self.mode,
                                "all_results": results[:5],
                                "class_methods": "BM25DualChunkEvaluator m√©todos individuales"
                            }
                except Exception as method_error:
                    print(f"‚ö†Ô∏è Error con {method_name}: {method_error}")
                    continue
            
            return None
            
        except Exception as e:
            print(f"‚ùå Error en fallback b√°sico: {e}")
            return None

    def _find_best_medical_chunk_bm25_fallback(self, question: str) -> Optional[Dict[str, Any]]:
        """Fallback usando m√©todo BM25 preparado de la clase"""
        try:
            print("üîÑ Usando calculate_bm25_rankings de la clase preparada...")
            bm25_results = self.retrieval_system.calculate_bm25_rankings(question)
            
            if not bm25_results:
                return None
            
            best_chunk = bm25_results[0]
            print(f"‚úÖ BM25 de clase preparada exitoso: {best_chunk}")
            
            chunk_text = self.retrieval_system.docs_raw.get(best_chunk, '')
            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk, {})
            
            return {
                "chunk_id": best_chunk,
                "text": chunk_text,
                "filename": chunk_metadata.get('filename', 'Gu√≠a m√©dica'),
                "document_id": chunk_metadata.get('document_id', ''),
                "chunk_position": chunk_metadata.get('chunk_position', ''),
                "categoria": chunk_metadata.get('categoria', 'medicina'),
                "strategy_used": f"BM25 de clase preparada (modelo {self.mode})",
                "model_type": self.mode,
                "all_results": bm25_results[:5],
                "class_methods": "BM25DualChunkEvaluator.calculate_bm25_rankings"
            }
            
        except Exception as e:
            print(f"‚ùå Error en BM25 de clase preparada: {e}")
            return None

    def _show_chunk_info_detailed(self, chunk_info: Dict[str, Any]):
        """Muestra informaci√≥n detallada del chunk encontrado"""
        print(f"‚úÖ INFORMACI√ìN M√âDICA ENCONTRADA:")
        print(f"   üéØ Estrategia: {chunk_info.get('strategy_used', 'No especificada')}")
        print(f"   ü§ñ Modelo: {chunk_info.get('model_type', 'No especificado')}")
        print(f"   üìÑ Documento: {chunk_info['document_id']}")
        print(f"   üìÇ Archivo: {chunk_info['filename']}")
        print(f"   üìç Posici√≥n: {chunk_info['chunk_position']}")
        print(f"   üè∑Ô∏è Categor√≠a: {chunk_info['categoria']}")
        print(f"   üìè Tama√±o: {len(chunk_info['text'])} caracteres")
        
        # Mostrar preview del contenido
        preview = chunk_info['text'][:350] + "..." if len(chunk_info['text']) > 350 else chunk_info['text']
        print(f"   üìù Contenido: {preview}")
        
        # Mostrar alternativas evaluadas
        print(f"   üîÑ Top 3 alternativas: {chunk_info['all_results'][:3]}")
        
        # Mostrar m√©todos de clase utilizados
        class_methods = chunk_info.get('class_methods', 'No especificado')
        pool_size = chunk_info.get('pool_size', 'N/A')
        
        print(f"   üèóÔ∏è M√©todos utilizados: {class_methods}")
        
        if 'preparada' in class_methods:
            print(f"   ‚úÖ Usando clase BM25DualChunkEvaluator con m√©todos preparados")
            if pool_size != 'N/A':
                print(f"   ‚ö° Pool optimizado: {pool_size} chunks")
        
        if 'finetuneado' in model_type:
            print(f"   üéØ Optimizaci√≥n: Modelo fine-tuneado m√©dico especializado")
            print(f"   ‚ö° Ventaja: Velocidad + Precisi√≥n + M√©todos experimentales preparados")
        else:
            print(f"   üí° M√©todo: Estrategia est√°ndar con m√©todos preparados")

    def _generate_professional_answer(self, question: str, chunk_info: Dict[str, Any]) -> str:
        """Genera respuesta m√©dica profesional y transparente"""
        
        chunk_text = chunk_info['text']
        source = chunk_info['filename']
        
        print("üí° Generando respuesta estructurada profesional")
        return self._create_professional_structured_answer(question, chunk_text, source)

    def _create_professional_structured_answer(self, question: str, context: str, source: str) -> str:
        """Crea respuesta estructurada profesional y transparente"""
        
        context_preview = context[:600] + "..." if len(context) > 600 else context
        question_lower = question.lower()
        
        # Introducci√≥n transparente y profesional
        professional_intro = "Bas√°ndome en la informaci√≥n m√©dica disponible en nuestras fuentes especializadas, puedo proporcionarle informaci√≥n relevante sobre su consulta."
        
        # Adaptar respuesta seg√∫n tipo de consulta
        if any(word in question_lower for word in ['dolor de cabeza', 'cefalea', 'migra√±a']):
            specific_advice = """
ü©∫ INFORMACI√ìN SOBRE CEFALEAS:
Los dolores de cabeza frecuentes pueden tener m√∫ltiples causas y requieren evaluaci√≥n m√©dica profesional para un diagn√≥stico preciso.

üìã RECOMENDACIONES INFORMATIVAS:
‚Ä¢ Considere mantener un diario de cefaleas: frecuencia, intensidad, duraci√≥n y posibles desencadenantes
‚Ä¢ Factores que pueden influir: patrones de sue√±o, hidrataci√≥n, estr√©s, alimentaci√≥n
‚Ä¢ La evaluaci√≥n m√©dica es importante para descartar causas subyacentes

‚ö†Ô∏è SIGNOS QUE REQUIEREN ATENCI√ìN M√âDICA URGENTE:
‚Ä¢ Dolor de cabeza severo y s√∫bito de inicio
‚Ä¢ Cefalea con fiebre, rigidez de cuello o alteraci√≥n de conciencia
‚Ä¢ Cambios en la visi√≥n, debilidad o dificultades del habla
‚Ä¢ Dolor que empeora progresivamente o patr√≥n inusual"""

        elif any(word in question_lower for word in ['diabetes', 'az√∫car', 'sed', 'orinar']):
            specific_advice = """
ü©∫ INFORMACI√ìN SOBRE S√çNTOMAS RELACIONADOS CON GLUCOSA:
Los s√≠ntomas que menciona pueden estar relacionados con alteraciones metab√≥licas y requieren evaluaci√≥n m√©dica.

üìã INFORMACI√ìN GENERAL:
‚Ä¢ Los an√°lisis de laboratorio son fundamentales para el diagn√≥stico
‚Ä¢ El registro de s√≠ntomas puede ser √∫til para la evaluaci√≥n m√©dica
‚Ä¢ El control metab√≥lico incluye aspectos diet√©ticos y de actividad f√≠sica

‚ö†Ô∏è SITUACIONES QUE REQUIEREN ATENCI√ìN M√âDICA INMEDIATA:
‚Ä¢ S√≠ntomas severos como n√°useas, v√≥mitos persistentes
‚Ä¢ Dificultad respiratoria o dolor abdominal intenso
‚Ä¢ Alteraci√≥n del estado de conciencia
‚Ä¢ Signos de deshidrataci√≥n severa"""

        elif any(word in question_lower for word in ['presi√≥n', 'tensi√≥n', 'hipertensi√≥n']):
            specific_advice = """
ü©∫ INFORMACI√ìN SOBRE PRESI√ìN ARTERIAL:
El control de la presi√≥n arterial es fundamental para la salud cardiovascular y requiere seguimiento m√©dico.

üìã INFORMACI√ìN GENERAL:
‚Ä¢ El diagn√≥stico de hipertensi√≥n requiere mediciones repetidas
‚Ä¢ El control incluye aspectos diet√©ticos, ejercicio y, a menudo, medicaci√≥n
‚Ä¢ El seguimiento m√©dico regular es esencial

‚ö†Ô∏è SITUACIONES QUE REQUIEREN ATENCI√ìN M√âDICA:
‚Ä¢ Cifras muy elevadas de presi√≥n arterial
‚Ä¢ S√≠ntomas como dolor de cabeza severo, mareos, problemas visuales
‚Ä¢ Dolor tor√°cico o dificultad respiratoria"""

        else:
            specific_advice = """
ü©∫ INFORMACI√ìN M√âDICA GENERAL:
Bas√°ndome en su consulta, puedo proporcionarle informaci√≥n general relevante.

üìã RECOMENDACIONES INFORMATIVAS:
‚Ä¢ Para una evaluaci√≥n personalizada, consulte con un profesional m√©dico
‚Ä¢ Mantenga un registro de s√≠ntomas si es relevante
‚Ä¢ Siga las medidas generales de promoci√≥n de la salud

‚ö†Ô∏è SIGNOS QUE REQUIEREN ATENCI√ìN M√âDICA:
‚Ä¢ S√≠ntomas que empeoran o persisten
‚Ä¢ Cualquier s√≠ntoma que le cause preocupaci√≥n
‚Ä¢ Cambios significativos en su estado de salud"""

        return f"""{professional_intro}

üìö INFORMACI√ìN M√âDICA RELEVANTE:
{context_preview}

{specific_advice}

üìû RECOMENDACIONES PARA SEGUIMIENTO:
‚Ä¢ Consulte con su m√©dico de cabecera para evaluaci√≥n personalizada
‚Ä¢ Proporcione informaci√≥n completa sobre sus s√≠ntomas y antecedentes
‚Ä¢ Siga las indicaciones m√©dicas profesionales

üí° *Esta informaci√≥n proviene de: {source}*

‚ÑπÔ∏è IMPORTANTE: Soy un asistente de informaci√≥n m√©dica que proporciona contenido educativo basado en fuentes confiables. Esta informaci√≥n no reemplaza la consulta m√©dica profesional. Para diagn√≥stico, tratamiento y recomendaciones personalizadas, es fundamental la evaluaci√≥n directa de un profesional de la salud."""

    def _show_final_response(self, response: OptimizedMedicalResponse):
        """Muestra la respuesta final de forma clara"""
        print(f"\nüî¨ RESPUESTA M√âDICA PROFESIONAL:")
        print("="*60)
        print(response.answer)
        print("="*60)
        print(f"‚è±Ô∏è Tiempo de procesamiento: {response.processing_time:.2f} segundos")
        print(f"‚úÖ Estado: {'Exitoso' if response.success else 'Error'}")
        print(f"üéØ Optimizaci√≥n aplicada: Modelo fine-tuneado m√©dico")


# ============ DEMOSTRACI√ìN OPTIMIZADA ============

def main():
    """Demostraci√≥n del RAG m√©dico optimizado"""
    
    print("üéØ RAG M√âDICO OPTIMIZADO CON MODELO FINE-TUNEADO - DEMOSTRACI√ìN")
    print("="*70)
    print("Optimizaciones: Modelo fine-tuneado + Pipeline h√≠brido + Pool reducido")
    print("Objetivo: M√°xima velocidad manteniendo calidad profesional")
    print("="*70)
    
    # Inicializar sistema optimizado
    rag = OptimizedMedicalRAG("../config.yaml", mode="finetuneado")
    
    if not rag.initialize():
        print("‚ùå Error en inicializaci√≥n")
        return
    
    # Consultas de prueba (las mismas que funcionaron bien)
    test_questions = [
        "¬øCu√°les son los s√≠ntomas de la diabetes?",
        "Doctor, tengo dolor de cabeza frecuente",
        "¬øQu√© puedo hacer para la presi√≥n alta?"
    ]
    
    print("üß™ PROBANDO CONSULTAS M√âDICAS OPTIMIZADAS:")
    print("="*50)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nüî¨ PRUEBA {i}/{len(test_questions)}")
        response = rag.ask_doctor(question)
        
        if not response.success:
            print(f"‚ùå Error en consulta: {response.answer}")
        
        # Pausa entre consultas para claridad
        if i < len(test_questions):
            input("\n‚è∏Ô∏è Presiona Enter para continuar con la siguiente consulta...")
    
    print(f"\nüéâ DEMOSTRACI√ìN COMPLETADA")
    print("Sistema optimizado que combina:")
    print("  üéØ Modelo fine-tuneado: Especializado en medicina")
    print("  üîç BM25: B√∫squeda por palabras clave")
    print("  üß† Bi-Encoder: Comprensi√≥n sem√°ntica mejorada") 
    print("  ‚ö° Cross-Encoder: Precisi√≥n final en pool reducido")
    print("  üìã Transparencia: No se hace pasar por m√©dico")
    print("  üî¨ Respuestas profesionales: Informaci√≥n confiable")

if __name__ == "__main__":
    main()