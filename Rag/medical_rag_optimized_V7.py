"""
medical_rag_simple_clear.py - RAG M√©dico Simple y Claro

OBJETIVO: Sistema f√°cil de entender sin confusi√≥n
PROCESO: Pregunta ‚Üí Pipeline H√≠brido ‚Üí Mejor Chunk ‚Üí Respuesta M√©dica
"""

import os
import sys
import logging
from typing import Dict, Any, Optional
from dataclasses import dataclass
import time

# Imports b√°sicos
import torch
from transformers import pipeline

# Importar la clase existente
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)

from retrieval.bm25_model_chunk_bge import BM25DualChunkEvaluator
from embeddings.load_model import cargar_configuracion

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

@dataclass
class SimpleMedicalResponse:
    """Respuesta m√©dica simple"""
    question: str
    answer: str
    chunk_used: Dict[str, Any]
    processing_time: float
    success: bool

class SimpleMedicalRAG:
    """
    RAG M√©dico Simple y Claro con Pipeline H√≠brido
    
    PROCESO SIMPLIFICADO:
    1. Usuario hace pregunta m√©dica
    2. Pipeline H√≠brido busca el chunk m√°s relevante (BM25 + Bi-Encoder + Cross-Encoder)
    3. Se muestra qu√© chunk encontr√≥
    4. Se genera respuesta m√©dica usando ese chunk
    5. Se muestra la respuesta final
    """
    
    def __init__(self, config_path: str, mode: str = "embedding"):
        """Inicializa RAG m√©dico simple"""
        
        self.config_path = config_path
        self.mode = mode
        
        try:
            self.config = cargar_configuracion(config_path)
        except Exception:
            logger.warning("‚ö†Ô∏è Usando configuraci√≥n por defecto")
            self.config = {}
        
        # Componentes del sistema
        self.retrieval_system = None
        self.generation_pipeline = None
        self.is_initialized = False
        
        logger.info("‚ö° RAG M√©dico Simple con Pipeline H√≠brido - Mejor calidad de recuperaci√≥n")

    def initialize(self) -> bool:
        """Inicializa el sistema de forma simple"""
        try:
            print("\nüîß INICIALIZANDO SISTEMA...")
            
            # 1. Cargar sistema BM25
            print("üìö Cargando base de conocimientos m√©dicos...")
            self.retrieval_system = BM25DualChunkEvaluator(self.config_path, self.mode)
            self.retrieval_system.load_collection()
            
            total_chunks = len(self.retrieval_system.chunk_ids)
            print(f"‚úÖ Base cargada: {total_chunks} fragmentos m√©dicos disponibles")
            
            # 2. Cargar modelo de generaci√≥n
            print("ü§ñ Cargando modelo de respuestas...")
            self._load_generation_model()
            
            self.is_initialized = True
            print("‚úÖ Sistema listo para consultas m√©dicas\n")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error inicializando: {e}")
            return False

    def _load_generation_model(self):
        """Carga modelo de generaci√≥n optimizado para medicina"""
        try:
            device = 0 if torch.cuda.is_available() else -1
            
            # Intentar modelos m√°s potentes primero
            model_candidates = [
                "microsoft/DialoGPT-large",    # Modelo conversacional potente
                "microsoft/DialoGPT-medium",   # Modelo intermedio
                "gpt2-large",                  # GPT-2 grande
                "gpt2"                         # Fallback b√°sico
            ]
            
            for model_name in model_candidates:
                try:
                    print(f"ü§ñ Probando modelo: {model_name}")
                    
                    self.generation_pipeline = pipeline(
                        "text-generation",
                        model=model_name,
                        device=device,
                        max_length=1024,  # Contexto amplio para prompts m√©dicos
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        trust_remote_code=True
                    )
                    
                    if self.generation_pipeline.tokenizer.pad_token is None:
                        self.generation_pipeline.tokenizer.pad_token = self.generation_pipeline.tokenizer.eos_token
                        
                    print(f"‚úÖ Modelo cargado exitosamente: {model_name}")
                    break
                    
                except Exception as model_error:
                    print(f"‚ö†Ô∏è Error con {model_name}: {model_error}")
                    continue
            
            if self.generation_pipeline is None:
                print("‚ö†Ô∏è No se pudo cargar ning√∫n modelo, usando respuestas estructuradas")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error general cargando modelos: {e}")
            print("üîÑ Usando modo respuestas estructuradas como m√©dico")
            self.generation_pipeline = None

    def ask_doctor(self, medical_question: str) -> SimpleMedicalResponse:
        """
        Pregunta al doctor - Proceso simple y claro
        """
        start_time = time.time()
        
        if not self.is_initialized:
            return SimpleMedicalResponse(
                question=medical_question,
                answer="Sistema no inicializado",
                chunk_used={},
                processing_time=0.0,
                success=False
            )
        
        print(f"\nüí¨ PREGUNTA: {medical_question}")
        print("="*60)
        
        try:
            # PASO 1: Buscar informaci√≥n m√©dica relevante
            print("üîç PASO 1: Buscando informaci√≥n m√©dica relevante...")
            chunk_info = self._find_best_medical_chunk(medical_question)
            
            if not chunk_info:
                return SimpleMedicalResponse(
                    question=medical_question,
                    answer="No encontr√© informaci√≥n m√©dica relevante para su consulta. Le recomiendo consultar con su m√©dico.",
                    chunk_used={},
                    processing_time=time.time() - start_time,
                    success=False
                )
            
            # PASO 2: Mostrar qu√© informaci√≥n se encontr√≥
            self._show_chunk_info(chunk_info)
            
            # PASO 3: Generar respuesta m√©dica
            print("\nü§ñ PASO 3: Generando respuesta m√©dica...")
            medical_answer = self._generate_medical_answer(medical_question, chunk_info)
            
            processing_time = time.time() - start_time
            
            response = SimpleMedicalResponse(
                question=medical_question,
                answer=medical_answer,
                chunk_used=chunk_info,
                processing_time=processing_time,
                success=True
            )
            
            # PASO 4: Mostrar respuesta final
            self._show_final_response(response)
            
            return response
            
        except Exception as e:
            print(f"‚ùå ERROR: {e}")
            return SimpleMedicalResponse(
                question=medical_question,
                answer=f"Error del sistema: {str(e)}",
                chunk_used={},
                processing_time=time.time() - start_time,
                success=False
            )

    def _find_best_medical_chunk(self, question: str) -> Optional[Dict[str, Any]]:
        """Busca el mejor fragmento m√©dico usando estrategia r√°pida: BM25 + Bi-Encoder (sin Cross-Encoder)"""
        try:
            print("‚ö° Usando estrategia r√°pida: BM25 + Bi-Encoder (sin Cross-Encoder lento)...")
            
            # ESTRATEGIA R√ÅPIDA: Solo BM25 + Bi-Encoder
            print("üîç Paso 1: Ranking BM25...")
            bm25_results = self.retrieval_system.calculate_bm25_rankings(question)
            
            print("üß† Paso 2: Ranking Bi-Encoder...")
            biencoder_results = self.retrieval_system.calculate_biencoder_rankings(question)
            
            # Crear pool balanceado peque√±o y r√°pido
            print("‚öñÔ∏è Paso 3: Pool balanceado r√°pido...")
            balanced_pool = self.retrieval_system.create_balanced_chunk_pool(
                bm25_results[:5],      # Solo top 5 de BM25
                biencoder_results[:5], # Solo top 5 de Bi-Encoder  
                pool_size=6            # Pool peque√±o para velocidad
            )
            
            if not balanced_pool:
                print("‚ö†Ô∏è Pool balanceado vac√≠o, usando solo BM25...")
                # Fallback a solo BM25
                balanced_pool = bm25_results[:1]  # Solo el mejor de BM25
            
            if not balanced_pool:
                return None
            
            # Sin Cross-Encoder - tomar el primer resultado del pool balanceado
            best_chunk = balanced_pool[0]
            print(f"üéØ Mejor chunk (BM25+Bi-Encoder): {best_chunk}")
            
            chunk_text = self.retrieval_system.docs_raw.get(best_chunk, '')
            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk, {})
            
            return {
                "chunk_id": best_chunk,
                "text": chunk_text,
                "filename": chunk_metadata.get('filename', 'Gu√≠a m√©dica'),
                "document_id": chunk_metadata.get('document_id', ''),
                "chunk_position": chunk_metadata.get('chunk_position', ''),
                "categoria": chunk_metadata.get('categoria', 'medicina'),
                "strategy_used": "BM25 + Bi-Encoder (sin Cross-Encoder)",
                "all_results": balanced_pool[:5]  # Top 5 para mostrar
            }
       
            
        except Exception as e:
            print(f"‚ùå Error en pipeline h√≠brido: {e}")
            print("üîÑ Fallback a BM25 simple...")
            return self._find_best_medical_chunk_bm25_only(question)

    def _find_best_medical_chunk_bm25_only(self, question: str) -> Optional[Dict[str, Any]]:
        """Fallback: Busca usando solo BM25 - Sin filtros, confianza en el algoritmo"""
        try:
            # Usar BM25 para encontrar candidatos
            bm25_results = self.retrieval_system.calculate_bm25_rankings(question)
            
            if not bm25_results:
                return None
            
            # Confiar en el primer resultado de BM25
            best_chunk = bm25_results[0]
            print(f"üéØ Mejor chunk seg√∫n BM25: {best_chunk}")
            
            chunk_text = self.retrieval_system.docs_raw.get(best_chunk, '')
            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk, {})
            
            return {
                "chunk_id": best_chunk,
                "text": chunk_text,
                "filename": chunk_metadata.get('filename', 'Gu√≠a m√©dica'),
                "document_id": chunk_metadata.get('document_id', ''),
                "chunk_position": chunk_metadata.get('chunk_position', ''),
                "categoria": chunk_metadata.get('categoria', 'medicina'),
                "strategy_used": "BM25 ranking directo",
                "all_results": bm25_results[:5]  # Top 5 para mostrar
            }
            
        except Exception as e:
            print(f"Error en b√∫squeda BM25: {e}")
            return None

    def _show_chunk_info(self, chunk_info: Dict[str, Any]):
        """Muestra informaci√≥n clara del chunk encontrado"""
        print(f"‚úÖ INFORMACI√ìN ENCONTRADA:")
        print(f"   üéØ Estrategia: {chunk_info.get('strategy_used', 'No especificada')}")
        print(f"   üìÑ Documento: {chunk_info['document_id']}")
        print(f"   üìÇ Archivo: {chunk_info['filename']}")
        print(f"   üìç Posici√≥n: {chunk_info['chunk_position']}")
        print(f"   üè∑Ô∏è Categor√≠a: {chunk_info['categoria']}")
        print(f"   üìè Tama√±o: {len(chunk_info['text'])} caracteres")
        
        # Mostrar preview del contenido
        preview = chunk_info['text'][:300] + "..." if len(chunk_info['text']) > 300 else chunk_info['text']
        print(f"   üìù Contenido: {preview}")
        
        # Mostrar alternativas consideradas
        print(f"   üîÑ Top 3 resultados: {chunk_info['all_results'][:3]}")
        
        # Mostrar ventaja de la estrategia
        strategy = chunk_info.get('strategy_used', '')
        if 'Bi-Encoder' in strategy and 'Cross-Encoder' not in strategy:
            print(f"   ‚ö° Ventaja: Combina BM25 (palabras) + Bi-Encoder (sem√°ntica) - R√ÅPIDO")
        elif 'BM25' in strategy and 'Bi-Encoder' not in strategy:
            print(f"   üí° M√©todo: Solo BM25 - ULTRARR√ÅPIDO")
        else:
            print(f"   üí° M√©todo: Estrategia de recuperaci√≥n personalizada")

    def _generate_medical_answer(self, question: str, chunk_info: Dict[str, Any]) -> str:
        """Genera respuesta m√©dica usando el chunk encontrado - SOLO respuestas estructuradas"""
        
        chunk_text = chunk_info['text']
        source = chunk_info['filename']
        
        # DECISI√ìN: Usar SOLO respuestas estructuradas que son m√°s confiables
        print("üí° Usando respuesta estructurada basada en fuentes m√©dicas confiables")
        return self._create_structured_answer(question, chunk_text, source)



    def _create_structured_answer(self, question: str, context: str, source: str) -> str:
        """Crea respuesta estructurada como asistente de informaci√≥n m√©dica basado en fuentes confiables"""
        
        context_preview = context[:500] + "..." if len(context) > 500 else context
        question_lower = question.lower()
        
        # Respuesta como asistente m√©dico basado en informaci√≥n confiable
        medical_intro = "Bas√°ndome en la informaci√≥n m√©dica disponible en nuestras fuentes confiables, puedo ayudarle con su consulta."
        
        # Adaptar respuesta seg√∫n tipo de consulta
        if any(word in question_lower for word in ['dolor de cabeza', 'cefalea', 'migra√±a']):
            specific_advice = """
ü©∫ EVALUACI√ìN INICIAL:
Los dolores de cabeza frecuentes requieren una evaluaci√≥n m√©dica adecuada para determinar su causa y el tratamiento m√°s apropiado.

üìã RECOMENDACIONES INMEDIATAS:
‚Ä¢ Mantenga un diario de cefaleas: anote cu√°ndo ocurren, intensidad (1-10), duraci√≥n y posibles desencadenantes
‚Ä¢ Aseg√∫rese de mantener una hidrataci√≥n adecuada y patrones de sue√±o regulares
‚Ä¢ Evite factores desencadenantes comunes como estr√©s, ayuno prolongado o ciertos alimentos

‚ö†Ô∏è SIGNOS DE ALARMA - BUSQUE ATENCI√ìN INMEDIATA SI PRESENTA:
‚Ä¢ Dolor de cabeza severo y s√∫bito ("el peor de su vida")
‚Ä¢ Cefalea acompa√±ada de fiebre, rigidez de cuello o alteraci√≥n de la conciencia
‚Ä¢ Cambios en la visi√≥n, debilidad o dificultad para hablar
‚Ä¢ Dolor de cabeza que empeora progresivamente"""

        elif any(word in question_lower for word in ['diabetes', 'az√∫car', 'sed', 'orinar']):
            specific_advice = """
ü©∫ EVALUACI√ìN INICIAL:
Los s√≠ntomas que describe pueden sugerir alteraciones en los niveles de glucosa y requieren evaluaci√≥n m√©dica.

üìã RECOMENDACIONES INMEDIATAS:
‚Ä¢ Programe una cita para realizarse an√°lisis de glucosa en sangre en ayunas
‚Ä¢ Mantenga un registro de s√≠ntomas: sed, micci√≥n frecuente, cambios en el apetito
‚Ä¢ Contin√∫e con una dieta equilibrada y ejercicio moderado seg√∫n su capacidad

‚ö†Ô∏è SIGNOS DE ALARMA - BUSQUE ATENCI√ìN INMEDIATA SI PRESENTA:
‚Ä¢ N√°useas o v√≥mitos persistentes
‚Ä¢ Dificultad para respirar o dolor abdominal intenso
‚Ä¢ Confusi√≥n o alteraci√≥n del nivel de conciencia
‚Ä¢ Deshidrataci√≥n severa"""

        else:
            specific_advice = """
ü©∫ EVALUACI√ìN INICIAL:
Bas√°ndome en su consulta y la informaci√≥n m√©dica disponible, le proporciono las siguientes recomendaciones.

üìã RECOMENDACIONES GENERALES:
‚Ä¢ Para una evaluaci√≥n personalizada, programe una cita en consulta
‚Ä¢ Mantenga un registro de sus s√≠ntomas para facilitar el diagn√≥stico
‚Ä¢ Siga las medidas generales de cuidado de la salud

‚ö†Ô∏è SIGNOS DE ALARMA - BUSQUE ATENCI√ìN INMEDIATA SI PRESENTA:
‚Ä¢ S√≠ntomas severos o que empeoran r√°pidamente
‚Ä¢ Dificultad respiratoria o dolor tor√°cico
‚Ä¢ Alteraci√≥n del nivel de conciencia o s√≠ntomas neurol√≥gicos"""

        return f"""{medical_intro}

üìö INFORMACI√ìN M√âDICA RELEVANTE:
{context_preview}

{specific_advice}

üìû PR√ìXIMOS PASOS:
‚Ä¢ Programe una cita con su m√©dico de cabecera para evaluaci√≥n presencial
‚Ä¢ Traiga consigo cualquier medicaci√≥n actual y resultados de estudios previos
‚Ä¢ No dude en contactar con profesionales m√©dicos si presenta s√≠ntomas de alarma

üí° *Esta respuesta est√° basada en: {source}*

‚ÑπÔ∏è IMPORTANTE: Soy un asistente de informaci√≥n m√©dica basado en fuentes confiables. Esta informaci√≥n es de car√°cter educativo y no reemplaza la consulta m√©dica profesional. Para un diagn√≥stico preciso y tratamiento personalizado, es fundamental la evaluaci√≥n cl√≠nica directa con un m√©dico."""

    def _show_final_response(self, response: SimpleMedicalResponse):
        """Muestra la respuesta final de forma clara"""
        print(f"\nüë®‚Äç‚öïÔ∏è RESPUESTA M√âDICA:")
        print("="*60)
        print(response.answer)
        print("="*60)
        print(f"‚è±Ô∏è Tiempo de procesamiento: {response.processing_time:.2f} segundos")
        print(f"‚úÖ Estado: {'Exitoso' if response.success else 'Error'}")


# ============ DEMOSTRACI√ìN SIMPLE ============

def main():
    """Demostraci√≥n simple del RAG m√©dico"""
    
    print("ü©∫ RAG M√âDICO R√ÅPIDO - DEMOSTRACI√ìN")
    print("="*50)
    print("Estrategia: BM25 + Bi-Encoder (sin Cross-Encoder lento)")
    print("Objetivo: Velocidad optimizada manteniendo calidad")
    print("="*50)
    
    # Inicializar sistema
    rag = SimpleMedicalRAG("../config.yaml", mode="embedding")
    
    if not rag.initialize():
        print("‚ùå Error en inicializaci√≥n")
        return
    
    # Consultas de prueba simples
    test_questions = [
        "¬øCu√°les son los s√≠ntomas de la diabetes?",
        "Doctor, tengo dolor de cabeza frecuente",
        "¬øQu√© puedo hacer para la presi√≥n alta?"
    ]
    
    print("üß™ PROBANDO CONSULTAS M√âDICAS:")
    print("="*40)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nüî¨ PRUEBA {i}/{len(test_questions)}")
        response = rag.ask_doctor(question)
        
        if not response.success:
            print(f"‚ùå Error en consulta: {response.answer}")
        
        # Pausa entre consultas para claridad
        if i < len(test_questions):
            input("\n‚è∏Ô∏è Presiona Enter para continuar con la siguiente consulta...")
    
    print(f"\nüéâ DEMOSTRACI√ìN COMPLETADA")
    print("Sistema r√°pido que combina:")
    print("  üîç BM25: B√∫squeda por palabras clave")
    print("  üß† Bi-Encoder: Comprensi√≥n sem√°ntica") 
    print("  ‚ö° Sin Cross-Encoder: Velocidad optimizada")
    print("  üìã Pool balanceado peque√±o: Eficiencia m√°xima")
    print("  üë®‚Äç‚öïÔ∏è Respuestas estructuradas: Calidad garantizada")

if __name__ == "__main__":
    main()