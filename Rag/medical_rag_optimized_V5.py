"""
medical_rag_simple_clear.py - RAG M√©dico Simple y Claro

OBJETIVO: Sistema f√°cil de entender sin confusi√≥n
PROCESO: Pregunta ‚Üí BM25 ‚Üí Mejor Chunk ‚Üí Respuesta M√©dica
"""

import os
import sys
import logging
from typing import Dict, Any, Optional
from dataclasses import dataclass
import time

# Imports b√°sicos
import torch
from transformers import pipeline

# Importar la clase existente
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)

from retrieval.bm25_model_chunk_bge import BM25DualChunkEvaluator
from embeddings.load_model import cargar_configuracion

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

@dataclass
class SimpleMedicalResponse:
    """Respuesta m√©dica simple"""
    question: str
    answer: str
    chunk_used: Dict[str, Any]
    processing_time: float
    success: bool

class SimpleMedicalRAG:
    """
    RAG M√©dico Simple y Claro
    
    PROCESO SIMPLIFICADO:
    1. Usuario hace pregunta m√©dica
    2. BM25 busca el chunk m√°s relevante
    3. Se muestra qu√© chunk encontr√≥
    4. Se genera respuesta m√©dica usando ese chunk
    5. Se muestra la respuesta final
    """
    
    def __init__(self, config_path: str, mode: str = "embedding"):
        """Inicializa RAG m√©dico simple"""
        
        self.config_path = config_path
        self.mode = mode
        
        try:
            self.config = cargar_configuracion(config_path)
        except Exception:
            logger.warning("‚ö†Ô∏è Usando configuraci√≥n por defecto")
            self.config = {}
        
        # Componentes del sistema
        self.retrieval_system = None
        self.generation_pipeline = None
        self.is_initialized = False
        
        print("ü©∫ RAG M√©dico Simple - F√°cil de entender")

    def initialize(self) -> bool:
        """Inicializa el sistema de forma simple"""
        try:
            print("\nüîß INICIALIZANDO SISTEMA...")
            
            # 1. Cargar sistema BM25
            print("üìö Cargando base de conocimientos m√©dicos...")
            self.retrieval_system = BM25DualChunkEvaluator(self.config_path, self.mode)
            self.retrieval_system.load_collection()
            
            total_chunks = len(self.retrieval_system.chunk_ids)
            print(f"‚úÖ Base cargada: {total_chunks} fragmentos m√©dicos disponibles")
            
            # 2. Cargar modelo de generaci√≥n
            print("ü§ñ Cargando modelo de respuestas...")
            self._load_generation_model()
            
            self.is_initialized = True
            print("‚úÖ Sistema listo para consultas m√©dicas\n")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error inicializando: {e}")
            return False

    def _load_generation_model(self):
        """Carga modelo de generaci√≥n optimizado para medicina"""
        try:
            device = 0 if torch.cuda.is_available() else -1
            
            # Intentar modelos m√°s potentes primero
            model_candidates = [
                "microsoft/DialoGPT-large",    # Modelo conversacional potente
                "microsoft/DialoGPT-medium",   # Modelo intermedio
                "gpt2-large",                  # GPT-2 grande
                "gpt2"                         # Fallback b√°sico
            ]
            
            for model_name in model_candidates:
                try:
                    print(f"ü§ñ Probando modelo: {model_name}")
                    
                    self.generation_pipeline = pipeline(
                        "text-generation",
                        model=model_name,
                        device=device,
                        max_length=1024,  # Contexto amplio para prompts m√©dicos
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        trust_remote_code=True
                    )
                    
                    if self.generation_pipeline.tokenizer.pad_token is None:
                        self.generation_pipeline.tokenizer.pad_token = self.generation_pipeline.tokenizer.eos_token
                        
                    print(f"‚úÖ Modelo cargado exitosamente: {model_name}")
                    break
                    
                except Exception as model_error:
                    print(f"‚ö†Ô∏è Error con {model_name}: {model_error}")
                    continue
            
            if self.generation_pipeline is None:
                print("‚ö†Ô∏è No se pudo cargar ning√∫n modelo, usando respuestas estructuradas")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error general cargando modelos: {e}")
            print("üîÑ Usando modo respuestas estructuradas como m√©dico")
            self.generation_pipeline = None

    def ask_doctor(self, medical_question: str) -> SimpleMedicalResponse:
        """
        Pregunta al doctor - Proceso simple y claro
        """
        start_time = time.time()
        
        if not self.is_initialized:
            return SimpleMedicalResponse(
                question=medical_question,
                answer="Sistema no inicializado",
                chunk_used={},
                processing_time=0.0,
                success=False
            )
        
        print(f"\nüí¨ PREGUNTA: {medical_question}")
        print("="*60)
        
        try:
            # PASO 1: Buscar informaci√≥n m√©dica relevante
            print("üîç PASO 1: Buscando informaci√≥n m√©dica relevante...")
            chunk_info = self._find_best_medical_chunk(medical_question)
            
            if not chunk_info:
                return SimpleMedicalResponse(
                    question=medical_question,
                    answer="No encontr√© informaci√≥n m√©dica relevante para su consulta. Le recomiendo consultar con su m√©dico.",
                    chunk_used={},
                    processing_time=time.time() - start_time,
                    success=False
                )
            
            # PASO 2: Mostrar qu√© informaci√≥n se encontr√≥
            self._show_chunk_info(chunk_info)
            
            # PASO 3: Generar respuesta m√©dica
            print("\nü§ñ PASO 3: Generando respuesta m√©dica...")
            medical_answer = self._generate_medical_answer(medical_question, chunk_info)
            
            processing_time = time.time() - start_time
            
            response = SimpleMedicalResponse(
                question=medical_question,
                answer=medical_answer,
                chunk_used=chunk_info,
                processing_time=processing_time,
                success=True
            )
            
            # PASO 4: Mostrar respuesta final
            self._show_final_response(response)
            
            return response
            
        except Exception as e:
            print(f"‚ùå ERROR: {e}")
            return SimpleMedicalResponse(
                question=medical_question,
                answer=f"Error del sistema: {str(e)}",
                chunk_used={},
                processing_time=time.time() - start_time,
                success=False
            )

    def _find_best_medical_chunk(self, question: str) -> Optional[Dict[str, Any]]:
        """Busca el mejor fragmento m√©dico usando BM25"""
        try:
            # Usar BM25 para encontrar el mejor chunk
            bm25_results = self.retrieval_system.calculate_bm25_rankings(question)
            
            if not bm25_results:
                return None
            
            # Tomar el mejor resultado
            best_chunk_id = bm25_results[0]
            chunk_text = self.retrieval_system.docs_raw.get(best_chunk_id, '')
            chunk_metadata = self.retrieval_system.metadatas.get(best_chunk_id, {})
            
            return {
                "chunk_id": best_chunk_id,
                "text": chunk_text,
                "filename": chunk_metadata.get('filename', 'Gu√≠a m√©dica'),
                "document_id": chunk_metadata.get('document_id', ''),
                "chunk_position": chunk_metadata.get('chunk_position', ''),
                "categoria": chunk_metadata.get('categoria', 'medicina'),
                "all_results": bm25_results[:5]  # Top 5 para mostrar
            }
            
        except Exception as e:
            print(f"Error en b√∫squeda BM25: {e}")
            return None

    def _show_chunk_info(self, chunk_info: Dict[str, Any]):
        """Muestra informaci√≥n clara del chunk encontrado"""
        print(f"‚úÖ INFORMACI√ìN ENCONTRADA:")
        print(f"   üìÑ Documento: {chunk_info['document_id']}")
        print(f"   üìÇ Archivo: {chunk_info['filename']}")
        print(f"   üìç Posici√≥n: {chunk_info['chunk_position']}")
        print(f"   üè∑Ô∏è Categor√≠a: {chunk_info['categoria']}")
        print(f"   üìè Tama√±o: {len(chunk_info['text'])} caracteres")
        
        # Mostrar preview del contenido
        preview = chunk_info['text'][:200] + "..." if len(chunk_info['text']) > 200 else chunk_info['text']
        print(f"   üìù Contenido: {preview}")
        
        # Mostrar alternativas consideradas
        print(f"   üéØ Alternativas BM25: {chunk_info['all_results'][:3]}")

    def _generate_medical_answer(self, question: str, chunk_info: Dict[str, Any]) -> str:
        """Genera respuesta m√©dica usando el chunk encontrado"""
        
        chunk_text = chunk_info['text']
        source = chunk_info['filename']
        
        # Intentar generaci√≥n con modelo si est√° disponible
        if self.generation_pipeline:
            generated = self._try_generate_with_model(question, chunk_text)
            if generated:
                return f"{generated}\n\nüí° *Informaci√≥n basada en: {source}*"
        
        # Respuesta estructurada como alternativa
        return self._create_structured_answer(question, chunk_text, source)

    def _try_generate_with_model(self, question: str, context: str) -> Optional[str]:
        """Intenta generar respuesta con el modelo usando prompt m√©dico profesional"""
        try:
            # PROMPT M√âDICO PROFESIONAL
            prompt = f"""Eres un m√©dico de atenci√≥n primaria con amplia experiencia cl√≠nica. Tu objetivo es proporcionar respuestas m√©dicas claras, emp√°ticas y basadas en evidencia cient√≠fica.

INFORMACI√ìN M√âDICA DISPONIBLE:
{context[:600]}

CONSULTA DEL PACIENTE:
{question}

INSTRUCCIONES:
- Responde como un m√©dico profesional y emp√°tico
- Basa tu respuesta en la informaci√≥n m√©dica proporcionada
- S√© claro y accesible para el paciente
- Incluye recomendaciones pr√°cticas cuando sea apropiado
- Indica cu√°ndo es necesario buscar atenci√≥n m√©dica urgente
- Mant√©n un tono tranquilizador pero profesional

RESPUESTA DEL M√âDICO:"""
            
            result = self.generation_pipeline(
                prompt,
                max_new_tokens=200,  # Respuestas m√°s completas
                temperature=0.3,     # Creatividad controlada para precisi√≥n m√©dica
                do_sample=True,
                repetition_penalty=1.1,  # Evitar repeticiones
                top_p=0.9,
                top_k=50,
                pad_token_id=self.generation_pipeline.tokenizer.eos_token_id,
                truncation=True
            )
            
            generated = result[0]['generated_text']
            
            # Extraer solo la respuesta del m√©dico
            if "RESPUESTA DEL M√âDICO:" in generated:
                answer = generated.split("RESPUESTA DEL M√âDICO:")[-1].strip()
            else:
                answer = generated[len(prompt):].strip()
            
            # Limpiar tokens especiales
            answer = answer.replace("</s>", "").replace("<|endoftext|>", "").strip()
            
            # Validar que la respuesta sea coherente y m√©dicamente apropiada
            if len(answer) > 30 and not self._has_excessive_repetition(answer) and self._is_medical_response(answer):
                return answer
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Error en generaci√≥n: {e}")
            
        return None

    def _is_medical_response(self, text: str) -> bool:
        """Verifica si la respuesta parece m√©dicamente apropiada"""
        medical_terms = [
            'paciente', 's√≠ntoma', 'diagn√≥stico', 'tratamiento', 'consulte', 
            'm√©dico', 'doctor', 'evaluaci√≥n', 'recomiendo', 'importante',
            'salud', 'cl√≠nico', 'atenci√≥n', 'cuidado'
        ]
        
        text_lower = text.lower()
        medical_content = sum(1 for term in medical_terms if term in text_lower)
        
        # Debe tener al menos 2 t√©rminos m√©dicos y estructura coherente
        has_structure = '.' in text or ',' in text
        return medical_content >= 2 and has_structure

    def _has_excessive_repetition(self, text: str) -> bool:
        """Detecta si hay repeticiones excesivas en el texto"""
        words = text.split()
        if len(words) < 5:
            return True
        
        # Verificar repeticiones consecutivas
        for i in range(len(words) - 2):
            if words[i] == words[i + 1] == words[i + 2]:
                return True
        
        return False

    def _create_structured_answer(self, question: str, context: str, source: str) -> str:
        """Crea respuesta estructurada como m√©dico de atenci√≥n primaria cuando falla el modelo"""
        
        context_preview = context[:500] + "..." if len(context) > 500 else context
        question_lower = question.lower()
        
        # Respuesta como m√©dico de atenci√≥n primaria
        medical_intro = "Como m√©dico de atenci√≥n primaria, comprendo su preocupaci√≥n y quiero ayudarle."
        
        # Adaptar respuesta seg√∫n tipo de consulta
        if any(word in question_lower for word in ['dolor de cabeza', 'cefalea', 'migra√±a']):
            specific_advice = """
ü©∫ EVALUACI√ìN INICIAL:
Los dolores de cabeza frecuentes requieren una evaluaci√≥n m√©dica adecuada para determinar su causa y el tratamiento m√°s apropiado.

üìã RECOMENDACIONES INMEDIATAS:
‚Ä¢ Mantenga un diario de cefaleas: anote cu√°ndo ocurren, intensidad (1-10), duraci√≥n y posibles desencadenantes
‚Ä¢ Aseg√∫rese de mantener una hidrataci√≥n adecuada y patrones de sue√±o regulares
‚Ä¢ Evite factores desencadenantes comunes como estr√©s, ayuno prolongado o ciertos alimentos

‚ö†Ô∏è SIGNOS DE ALARMA - BUSQUE ATENCI√ìN INMEDIATA SI PRESENTA:
‚Ä¢ Dolor de cabeza severo y s√∫bito ("el peor de su vida")
‚Ä¢ Cefalea acompa√±ada de fiebre, rigidez de cuello o alteraci√≥n de la conciencia
‚Ä¢ Cambios en la visi√≥n, debilidad o dificultad para hablar
‚Ä¢ Dolor de cabeza que empeora progresivamente"""

        elif any(word in question_lower for word in ['diabetes', 'az√∫car', 'sed', 'orinar']):
            specific_advice = """
ü©∫ EVALUACI√ìN INICIAL:
Los s√≠ntomas que describe pueden sugerir alteraciones en los niveles de glucosa y requieren evaluaci√≥n m√©dica.

üìã RECOMENDACIONES INMEDIATAS:
‚Ä¢ Programe una cita para realizarse an√°lisis de glucosa en sangre en ayunas
‚Ä¢ Mantenga un registro de s√≠ntomas: sed, micci√≥n frecuente, cambios en el apetito
‚Ä¢ Contin√∫e con una dieta equilibrada y ejercicio moderado seg√∫n su capacidad

‚ö†Ô∏è SIGNOS DE ALARMA - BUSQUE ATENCI√ìN INMEDIATA SI PRESENTA:
‚Ä¢ N√°useas o v√≥mitos persistentes
‚Ä¢ Dificultad para respirar o dolor abdominal intenso
‚Ä¢ Confusi√≥n o alteraci√≥n del nivel de conciencia
‚Ä¢ Deshidrataci√≥n severa"""

        else:
            specific_advice = """
ü©∫ EVALUACI√ìN INICIAL:
Bas√°ndome en su consulta y la informaci√≥n m√©dica disponible, le proporciono las siguientes recomendaciones.

üìã RECOMENDACIONES GENERALES:
‚Ä¢ Para una evaluaci√≥n personalizada, programe una cita en consulta
‚Ä¢ Mantenga un registro de sus s√≠ntomas para facilitar el diagn√≥stico
‚Ä¢ Siga las medidas generales de cuidado de la salud

‚ö†Ô∏è SIGNOS DE ALARMA - BUSQUE ATENCI√ìN INMEDIATA SI PRESENTA:
‚Ä¢ S√≠ntomas severos o que empeoran r√°pidamente
‚Ä¢ Dificultad respiratoria o dolor tor√°cico
‚Ä¢ Alteraci√≥n del nivel de conciencia o s√≠ntomas neurol√≥gicos"""

        return f"""{medical_intro}

üìö INFORMACI√ìN M√âDICA RELEVANTE:
{context_preview}

{specific_advice}

üìû PR√ìXIMOS PASOS:
‚Ä¢ Programe una cita en consulta para evaluaci√≥n presencial
‚Ä¢ Traiga consigo cualquier medicaci√≥n actual y resultados de estudios previos
‚Ä¢ No dude en contactar si presenta s√≠ntomas de alarma

üí° *Esta respuesta est√° basada en: {source}*

‚ÑπÔ∏è Recuerde: Esta informaci√≥n es de car√°cter educativo y no reemplaza la consulta m√©dica presencial. Para un diagn√≥stico preciso y tratamiento personalizado, es fundamental la evaluaci√≥n cl√≠nica directa."""

    def _show_final_response(self, response: SimpleMedicalResponse):
        """Muestra la respuesta final de forma clara"""
        print(f"\nüë®‚Äç‚öïÔ∏è RESPUESTA M√âDICA:")
        print("="*60)
        print(response.answer)
        print("="*60)
        print(f"‚è±Ô∏è Tiempo de procesamiento: {response.processing_time:.2f} segundos")
        print(f"‚úÖ Estado: {'Exitoso' if response.success else 'Error'}")


# ============ DEMOSTRACI√ìN SIMPLE ============

def main():
    """Demostraci√≥n simple del RAG m√©dico"""
    
    print("ü©∫ RAG M√âDICO SIMPLE - DEMOSTRACI√ìN")
    print("="*50)
    print("Objetivo: Sistema f√°cil de entender")
    print("="*50)
    
    # Inicializar sistema
    rag = SimpleMedicalRAG("../config.yaml", mode="embedding")
    
    if not rag.initialize():
        print("‚ùå Error en inicializaci√≥n")
        return
    
    # Consultas de prueba simples
    test_questions = [
        "¬øCu√°les son los s√≠ntomas de la diabetes?",
        "Doctor, tengo dolor de cabeza frecuente",
        "¬øQu√© puedo hacer para la presi√≥n alta?"
    ]
    
    print("üß™ PROBANDO CONSULTAS M√âDICAS:")
    print("="*40)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nüî¨ PRUEBA {i}/{len(test_questions)}")
        response = rag.ask_doctor(question)
        
        if not response.success:
            print(f"‚ùå Error en consulta: {response.answer}")
        
        # Pausa entre consultas para claridad
        if i < len(test_questions):
            input("\n‚è∏Ô∏è Presiona Enter para continuar con la siguiente consulta...")
    
    print(f"\nüéâ DEMOSTRACI√ìN COMPLETADA")
    print("Sistema simple que muestra claramente:")
    print("  üìö Qu√© informaci√≥n encontr√≥")
    print("  üîç De d√≥nde viene la informaci√≥n") 
    print("  ü§ñ C√≥mo genera la respuesta")
    print("  üë®‚Äç‚öïÔ∏è La respuesta m√©dica final")

if __name__ == "__main__":
    main()